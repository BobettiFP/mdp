# ------------------------------  improved_train.py  -----------------------------------
"""
ê°œì„ ëœ PPO í•™ìŠµ - ë‹¤ì–‘í•œ ë‚œì´ë„ì˜ í™˜ê²½ì—ì„œ ë¹„êµ ì‹¤í—˜
"""
import argparse, os
from typing import List, Tuple, Dict
import numpy as np, pandas as pd
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import BaseCallback
from improved_env import build_improved_env

class DetailedEpisodeLogger(BaseCallback):
    """ë” ìƒì„¸í•œ ì—í”¼ì†Œë“œ ë¡œê¹…"""
    def __init__(self, verbose=0):
        super().__init__(verbose)
        self.episode_rewards = []
        self.episode_successes = []
        self.episode_lengths = []
        self.episode_info = []
        self.current_episode_reward = 0
        self.current_episode_success = 0
        self.current_episode_length = 0
        
    def _on_step(self) -> bool:
        self.current_episode_reward += self.locals['rewards'][0]
        self.current_episode_length += 1
        
        if self.locals['rewards'][0] > 0:
            self.current_episode_success = 1
            
        if self.locals['dones'][0]:
            self.episode_rewards.append(self.current_episode_reward)
            self.episode_successes.append(self.current_episode_success)
            self.episode_lengths.append(self.current_episode_length)
            
            # ì¶”ê°€ ì •ë³´ ì €ì¥
            info = self.locals.get('infos', [{}])[0]
            self.episode_info.append(info)
            
            # ë¦¬ì…‹
            self.current_episode_reward = 0
            self.current_episode_success = 0
            self.current_episode_length = 0
            
        return True

def run_experiment(env, steps: int, env_name: str) -> Dict:
    """ë‹¨ì¼ í™˜ê²½ì—ì„œ ì‹¤í—˜ ì‹¤í–‰"""
    print(f"\n{'='*50}")
    print(f"ğŸš€ Starting {env_name} experiment")
    print(f"{'='*50}")
    print(f"Obs space: {env.observation_space}")
    print(f"Action space: {env.action_space}")
    
    # PPO ëª¨ë¸ - í™˜ê²½ì— ë”°ë¼ ë‹¤ë¥¸ ì„¤ì •
    if "hard" in env_name:
        learning_rate = 1e-4  # ì–´ë ¤ìš´ í™˜ê²½ì€ ë‚®ì€ í•™ìŠµë¥ 
        n_epochs = 15
    elif "easy" in env_name:
        learning_rate = 5e-4  # ì‰¬ìš´ í™˜ê²½ì€ ë†’ì€ í•™ìŠµë¥ 
        n_epochs = 5
    else:
        learning_rate = 3e-4  # ê¸°ë³¸ê°’
        n_epochs = 10
    
    model = PPO(
        "MlpPolicy", 
        env,
        verbose=1,
        learning_rate=learning_rate,
        n_steps=min(2048, steps // 4),
        batch_size=64,
        n_epochs=n_epochs,
        gamma=0.99,
        gae_lambda=0.95,
        clip_range=0.2,
        ent_coef=0.01,
        tensorboard_log=f"./tensorboard/{env_name}/"
    )
    
    logger = DetailedEpisodeLogger()
    
    try:
        print(f"Training for {steps:,} steps...")
        model.learn(
            total_timesteps=steps,
            callback=logger,
            progress_bar=False
        )
        
        print(f"âœ… Training completed: {len(logger.episode_rewards)} episodes")
        
        # ì¶”ê°€ í‰ê°€ ì—í”¼ì†Œë“œ
        if len(logger.episode_rewards) < 20:
            print("Collecting evaluation episodes...")
            for _ in range(30):
                obs, _ = env.reset()
                episode_reward = 0
                episode_success = 0
                episode_length = 0
                
                for _ in range(50):
                    action, _ = model.predict(obs, deterministic=True)
                    obs, reward, done, truncated, info = env.step(action)
                    episode_reward += reward
                    episode_length += 1
                    
                    if reward > 0:
                        episode_success = 1
                    if done or truncated:
                        break
                
                logger.episode_rewards.append(episode_reward)
                logger.episode_successes.append(episode_success)
                logger.episode_lengths.append(episode_length)
        
    except Exception as e:
        print(f"âŒ Training failed: {e}")
        # ëœë¤ ì •ì±…ìœ¼ë¡œ ê¸°ë³¸ ë°ì´í„° ìˆ˜ì§‘
        print("Collecting baseline data with random policy...")
        for _ in range(20):
            obs, _ = env.reset()
            episode_reward = 0
            episode_success = 0
            episode_length = 0
            
            for _ in range(20):
                action = env.action_space.sample()
                obs, reward, done, truncated, _ = env.step(action)
                episode_reward += reward
                episode_length += 1
                
                if reward > 0:
                    episode_success = 1
                if done or truncated:
                    break
            
            logger.episode_rewards.append(episode_reward)
            logger.episode_successes.append(episode_success)
            logger.episode_lengths.append(episode_length)
    
    # ê²°ê³¼ ì •ë¦¬
    if not logger.episode_rewards:
        logger.episode_rewards = [0.0]
        logger.episode_successes = [0]
        logger.episode_lengths = [1]
    
    results = {
        'rewards': logger.episode_rewards,
        'successes': logger.episode_successes,
        'lengths': logger.episode_lengths,
        'episodes': len(logger.episode_rewards),
        'avg_reward': np.mean(logger.episode_rewards),
        'avg_success': np.mean(logger.episode_successes) * 100,
        'avg_length': np.mean(logger.episode_lengths),
        'std_reward': np.std(logger.episode_rewards)
    }
    
    print(f"ğŸ“Š {env_name} Results:")
    print(f"   Episodes: {results['episodes']}")
    print(f"   Avg Reward: {results['avg_reward']:.3f} (Â±{results['std_reward']:.3f})")
    print(f"   Success Rate: {results['avg_success']:.1f}%")
    print(f"   Avg Episode Length: {results['avg_length']:.1f}")
    
    return results

def save_detailed_results(results: Dict, output_dir: str):
    """ìƒì„¸ ê²°ê³¼ ì €ì¥"""
    os.makedirs(output_dir, exist_ok=True)
    
    # í™˜ê²½ë³„ CSV ì €ì¥
    for env_name, data in results.items():
        if isinstance(data, dict) and 'rewards' in data:
            df = pd.DataFrame({
                'episode': range(1, len(data['rewards']) + 1),
                'reward': data['rewards'],
                'success': data['successes'],
                'length': data['lengths']
            })
            
            output_path = os.path.join(output_dir, f"{env_name}.csv")
            df.to_csv(output_path, index=False)
            print(f"ğŸ’¾ Saved {output_path}")
    
    # ìš”ì•½ ë¦¬í¬íŠ¸ ì €ì¥
    summary_data = []
    for env_name, data in results.items():
        if isinstance(data, dict) and 'avg_reward' in data:
            summary_data.append({
                'Environment': env_name,
                'Episodes': data['episodes'],
                'Avg_Reward': data['avg_reward'],
                'Std_Reward': data['std_reward'],
                'Success_Rate': data['avg_success'],
                'Avg_Length': data['avg_length']
            })
    
    if summary_data:
        summary_df = pd.DataFrame(summary_data)
        summary_path = os.path.join(output_dir, "experiment_summary.csv")
        summary_df.to_csv(summary_path, index=False)
        print(f"ğŸ“‹ Saved summary: {summary_path}")

def main():
    parser = argparse.ArgumentParser(description="ê°œì„ ëœ ëŒ€í™” í™˜ê²½ ì‹¤í—˜")
    parser.add_argument("--annotations", default="processed_annotations.json")
    parser.add_argument("--steps", type=int, default=20000)
    parser.add_argument("--outdir", default="improved_logs")
    parser.add_argument("--experiments", nargs='+', 
                       default=["human_normal", "human_hard", "llm_normal", "llm_hard"],
                       help="ì‹¤í—˜í•  í™˜ê²½ë“¤")
    
    args = parser.parse_args()
    
    print(f"ğŸ§ª Improved Dialogue RL Experiments")
    print(f"Annotations: {args.annotations}")
    print(f"Steps per experiment: {args.steps:,}")
    print(f"Output directory: {args.outdir}")
    print(f"Experiments: {args.experiments}")
    
    results = {}
    
    for exp_name in args.experiments:
        # ì‹¤í—˜ ì„¤ì • íŒŒì‹±
        if "_" in exp_name:
            ann_type, difficulty = exp_name.split("_", 1)
        else:
            ann_type, difficulty = exp_name, "normal"
        
        try:
            # í™˜ê²½ ìƒì„±
            env = build_improved_env(args.annotations, ann_type, difficulty)
            
            # ì‹¤í—˜ ì‹¤í–‰
            exp_results = run_experiment(env, args.steps, exp_name)
            results[exp_name] = exp_results
            
        except Exception as e:
            print(f"âŒ Experiment {exp_name} failed: {e}")
            results[exp_name] = {"error": str(e)}
    
    # ê²°ê³¼ ì €ì¥
    save_detailed_results(results, args.outdir)
    
    # ìµœì¢… ìš”ì•½
    print(f"\n{'='*60}")
    print("ğŸ EXPERIMENT SUMMARY")
    print(f"{'='*60}")
    
    for exp_name, data in results.items():
        if "error" in data:
            print(f"{exp_name:>15}: âŒ {data['error']}")
        else:
            print(f"{exp_name:>15}: {data['episodes']:>3} episodes, "
                  f"reward {data['avg_reward']:>6.3f}Â±{data['std_reward']:.3f}, "
                  f"success {data['avg_success']:>5.1f}%, "
                  f"length {data['avg_length']:>4.1f}")
    
    # ìµœê³  ì„±ëŠ¥ í™˜ê²½ ì°¾ê¸°
    valid_results = {k: v for k, v in results.items() if "error" not in v}
    if valid_results:
        best_env = max(valid_results.keys(), 
                      key=lambda k: valid_results[k]['avg_reward'] + valid_results[k]['avg_success']/100)
        print(f"\nğŸ† Best performing environment: {best_env}")

if __name__ == "__main__":
    main()