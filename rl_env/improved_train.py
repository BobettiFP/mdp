# ------------------------------  improved_train.py  -----------------------------------
"""
ê°œì„ ëœ PPO í•™ìŠµ - ë‹¤ì–‘í•œ ë‚œì´ë„ì˜ í™˜ê²½ì—ì„œ ë¹„êµ ì‹¤í—˜
"""
import argparse, os, json, pathlib as p
from typing import List, Tuple, Dict

import numpy as np
import pandas as pd
import gymnasium as gym                       # NEW
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import BaseCallback

from improved_env import build_improved_env
LOG_DIR = "step_logs"          # â† ìŠ¤í…â€‘ë‹¨ìœ„ ë¡œê·¸ê°€ ìŒ“ì¼ í´ë”
p.Path(LOG_DIR).mkdir(exist_ok=True)

# --------------------------------------------------------------------
# NEW: step-ë‹¨ìœ„ transitionì„ JSONLë¡œ ê¸°ë¡í•˜ëŠ” ë˜í¼
# --------------------------------------------------------------------
class TransitionLogger(gym.Wrapper):
    """
    env.step() í˜¸ì¶œë§ˆë‹¤
    {episode, t, state_before, action, state_after, reward}
    í•œ ì¤„(JSON)ì”© ê¸°ë¡.
    """
    def __init__(self, env: gym.Env, path: str, flush_every: int = 5000):
        super().__init__(env)
        self.path = path
        self.flush_every = flush_every
        self.buffer = []
        self.episode = 0
        self.t = 0

    # Gymnasium API
    def reset(self, **kwargs):
        obs, info = self.env.reset(**kwargs)
        self.episode += 1
        self.t = 0
        return obs, info

    def step(self, action):
        state_before = tuple(self.env._current_state)
        obs, reward, done, truncated, info = self.env.step(action)
        state_after  = tuple(self.env._current_state)
        self.t += 1

        self.buffer.append({
            "episode": self.episode,
            "t": self.t,
            "state_before": state_before,
            "action": int(action),
            "state_after": state_after,
            "reward": float(reward)
        })

        if done or truncated or len(self.buffer) >= self.flush_every:
            self._flush()
        return obs, reward, done, truncated, info

    def _flush(self):
        if not self.buffer:
            return
        with open(self.path, "a", encoding="utf-8") as f:
            for rec in self.buffer:
                f.write(json.dumps(rec, ensure_ascii=False) + "\n")
        self.buffer.clear()


class DetailedEpisodeLogger(BaseCallback):
    """ë” ìƒì„¸í•œ ì—í”¼ì†Œë“œ ë¡œê¹… (ê¸°ì¡´ ì½”ë“œ ê·¸ëŒ€ë¡œ)"""
    def __init__(self, verbose=0):
        super().__init__(verbose)
        self.episode_rewards, self.episode_successes, self.episode_lengths = [], [], []

    def _on_step(self) -> bool:
        # Stable-Baselines3 callback API - ì—¬ê¸°ì„  ë³„ë„ ì‘ì—… ì•ˆ í•¨
        return True


# --------------------------------------------------------------------
# ìˆ˜ì •: output_dir ì¸ìë¥¼ ë°›ë„ë¡ ë³€ê²½
# --------------------------------------------------------------------
def run_experiment(env, steps: int, env_name: str, output_dir: str) -> Dict:
    """ë‹¨ì¼ í™˜ê²½ì—ì„œ ì‹¤í—˜ ì‹¤í–‰"""
    print(f"\n{'='*50}")
    print(f"ğŸš€ Starting {env_name} experiment")
    print(f"{'='*50}")
    print(f"Obs space: {env.observation_space}")
    print(f"Action space: {env.action_space}")

    # PPO í•˜ì´í¼íŒŒë¼ë¯¸í„°
    if "hard" in env_name:
        learning_rate, n_epochs = 1e-4, 15
    elif "easy" in env_name:
        learning_rate, n_epochs = 5e-4, 5
    else:
        learning_rate, n_epochs = 3e-4, 10

    model = PPO(
        "MlpPolicy",
        env,
        learning_rate=learning_rate,
        n_epochs=n_epochs,
        batch_size=256,
        verbose=0
    )

    logger = DetailedEpisodeLogger()

    # NEW: transition ë¡œê·¸ íŒŒì¼ ê²½ë¡œ ì§€ì • + env ë˜í•‘
    trans_path = os.path.join(output_dir, f"{env_name}_transitions.jsonl")
    env = TransitionLogger(env, trans_path)

    try:
        print(f"Training for {steps:,} steps...")
        model.learn(total_timesteps=steps, callback=logger, progress_bar=False)
        env._flush()                                   # ë‚¨ì€ ë²„í¼ í”ŒëŸ¬ì‹œ
        print(f"âœ… Training completed: {len(logger.episode_rewards)} episodes")

    except Exception as e:
        print(f"âŒ Training failed: {e}")
        # ì‹¤íŒ¨ ì‹œ ëœë¤ ì •ì±…ìœ¼ë¡œ ìµœì†Œí•œì˜ ë°ì´í„° ìˆ˜ì§‘
        for _ in range(20):
            obs, _ = env.reset()
            ep_rew = ep_len = ep_succ = 0
            for _ in range(20):
                action = env.action_space.sample()
                obs, rew, done, trunc, _ = env.step(action)
                ep_rew += rew
                ep_len += 1
                if rew > 0:
                    ep_succ = 1
                if done or trunc:
                    break
            logger.episode_rewards.append(ep_rew)
            logger.episode_successes.append(ep_succ)
            logger.episode_lengths.append(ep_len)

    # ê²°ê³¼ ì§‘ê³„
    if not logger.episode_rewards:
        logger.episode_rewards, logger.episode_successes, logger.episode_lengths = [0.0], [0], [1]

    results = {
        'rewards': logger.episode_rewards,
        'successes': logger.episode_successes,
        'lengths': logger.episode_lengths,
        'episodes': len(logger.episode_rewards),
        'avg_reward': np.mean(logger.episode_rewards),
        'avg_success': np.mean(logger.episode_successes) * 100,
        'avg_length': np.mean(logger.episode_lengths),
        'std_reward': np.std(logger.episode_rewards)
    }

    print(f"ğŸ“Š {env_name} Results:"
          f" episodes {results['episodes']},"
          f" avg_reward {results['avg_reward']:.3f},"
          f" success {results['avg_success']:.1f}%,"
          f" len {results['avg_length']:.1f}")
    return results


def save_detailed_results(results: Dict, output_dir: str):
    """ìƒì„¸ ê²°ê³¼ ì €ì¥"""
    os.makedirs(output_dir, exist_ok=True)

    # í™˜ê²½ë³„ CSV
    for env_name, data in results.items():
        if isinstance(data, dict) and 'rewards' in data:
            pd.DataFrame({
                'episode': range(1, len(data['rewards']) + 1),
                'reward': data['rewards'],
                'success': data['successes'],
                'length': data['lengths']
            }).to_csv(os.path.join(output_dir, f"{env_name}.csv"), index=False)

    # ìš”ì•½ ë¦¬í¬íŠ¸
    summary = [{
        'Environment': n,
        'Episodes': d['episodes'],
        'Avg_Reward': d['avg_reward'],
        'Std_Reward': d['std_reward'],
        'Success_Rate': d['avg_success'],
        'Avg_Length': d['avg_length']
    } for n, d in results.items() if 'avg_reward' in d]
    if summary:
        pd.DataFrame(summary).to_csv(os.path.join(output_dir, "experiment_summary.csv"), index=False)


def main():
    parser = argparse.ArgumentParser(description="ê°œì„ ëœ ëŒ€í™” í™˜ê²½ ì‹¤í—˜")
    parser.add_argument("--annotations", default="processed_annotations.json")
    parser.add_argument("--steps", type=int, default=20000)
    parser.add_argument("--outdir", default="improved_logs")
    parser.add_argument("--experiments", nargs='+',
                        default=["human_normal", "human_hard", "llm_normal", "llm_hard"],
                        help="ì‹¤í—˜í•  í™˜ê²½ë“¤")
    args = parser.parse_args()

    print(f"ğŸ§ª Improved Dialogue RL Experiments")
    print(f"Annotations: {args.annotations}")
    print(f"Steps per experiment: {args.steps:,}")
    print(f"Output directory: {args.outdir}")
    print(f"Experiments: {args.experiments}")

    results = {}
    for exp_name in args.experiments:
        ann_type, difficulty = (exp_name.split("_", 1) + ["normal"])[:2]
        try:
            env = build_improved_env(args.annotations, ann_type, difficulty)
            results[exp_name] = run_experiment(env, args.steps, exp_name, args.outdir)
        except Exception as e:
            print(f"âŒ {exp_name} failed: {e}")
            results[exp_name] = {"error": str(e)}

    save_detailed_results(results, args.outdir)

    # ìµœê³  ì„±ëŠ¥ í™˜ê²½ í‘œì‹œ
    best = {k: v for k, v in results.items() if 'avg_reward' in v}
    if best:
        top = max(best, key=lambda k: best[k]['avg_reward'] + best[k]['avg_success'] / 100)
        print(f"\nğŸ† Best performing environment: {top}")


if __name__ == "__main__":
    main()
