# ------------------------------  train.py  -----------------------------------
"""
Train PPO agent on human vs LLM envs and write episode logs.
"""
import argparse, os
from typing import List, Tuple
import numpy as np, pandas as pd
from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.callbacks import BaseCallback
from env import build_env

class EpisodeLogger(BaseCallback):
    """Callback to log episode statistics."""
    def __init__(self, verbose=0):
        super(EpisodeLogger, self).__init__(verbose)
        self.episode_rewards = []
        self.episode_successes = []
        self.current_episode_reward = 0
        self.current_episode_success = 0
        
    def _on_step(self) -> bool:
        # ì—í”¼ì†Œë“œ ì§„í–‰ ì¤‘ ëˆ„ì 
        self.current_episode_reward += self.locals['rewards'][0]
        if self.locals['rewards'][0] > 0:
            self.current_episode_success = 1
            
        # ì—í”¼ì†Œë“œ ì¢…ë£Œ ì‹œ ê¸°ë¡
        if self.locals['dones'][0]:
            self.episode_rewards.append(self.current_episode_reward)
            self.episode_successes.append(self.current_episode_success)
            self.current_episode_reward = 0
            self.current_episode_success = 0
            
        return True

def run(env, steps: int) -> Tuple[List[float], List[int]]:
    """Train PPO agent and collect episode statistics."""
    print(f"Environment obs space: {env.observation_space}")
    print(f"Environment action space: {env.action_space}")
    
    # PPO ëª¨ë¸ ì„¤ì • ê°œì„ 
    model = PPO(
        "MlpPolicy", 
        env,
        verbose=1,
        learning_rate=3e-4,
        n_steps=min(2048, steps // 4),  # ìŠ¤í… ìˆ˜ì— ë§ê²Œ ì¡°ì •
        batch_size=64,
        n_epochs=10,
        gamma=0.99,
        gae_lambda=0.95,
        clip_range=0.2,
        ent_coef=0.01,
        tensorboard_log="./ppo_dialogue_tensorboard/"
    )
    
    # Episode logger ì„¤ì •
    logger = EpisodeLogger()
    
    try:
        # í•™ìŠµ ì‹¤í–‰ (progress_bar ì œê±°ë¡œ tqdm ì˜ì¡´ì„± í•´ê²°)
        print(f"Starting training for {steps} steps...")
        model.learn(
            total_timesteps=steps,
            callback=logger,
            progress_bar=False  # progress bar ë¹„í™œì„±í™”
        )
        
        print(f"Training completed. Collected {len(logger.episode_rewards)} episodes.")
        
        # ì—í”¼ì†Œë“œê°€ ë„ˆë¬´ ì ìœ¼ë©´ ì¶”ê°€ ìˆ˜ì§‘
        if len(logger.episode_rewards) < 10:
            print("Collecting additional episodes for evaluation...")
            for _ in range(20):  # ì¶”ê°€ ì—í”¼ì†Œë“œ ìˆ˜ì§‘
                obs, _ = env.reset()
                episode_reward = 0
                episode_success = 0
                done = False
                
                while not done:
                    action, _ = model.predict(obs, deterministic=False)
                    obs, reward, done, truncated, _ = env.step(action)
                    episode_reward += reward
                    if reward > 0:
                        episode_success = 1
                    if truncated:
                        break
                
                logger.episode_rewards.append(episode_reward)
                logger.episode_successes.append(episode_success)
        
    except Exception as e:
        print(f"Training failed: {e}")
        print("Attempting manual episode collection...")
        
        # í•™ìŠµ ì‹¤íŒ¨ ì‹œ ìˆ˜ë™ìœ¼ë¡œ ì—í”¼ì†Œë“œ ìˆ˜ì§‘
        try:
            for i in range(10):
                obs, _ = env.reset()
                episode_reward = 0
                episode_success = 0
                
                for _ in range(50):  # ìµœëŒ€ 50 ìŠ¤í…
                    action = env.action_space.sample()  # ëœë¤ ì•¡ì…˜
                    obs, reward, done, truncated, _ = env.step(action)
                    episode_reward += reward
                    if reward > 0:
                        episode_success = 1
                    if done or truncated:
                        break
                
                logger.episode_rewards.append(episode_reward)
                logger.episode_successes.append(episode_success)
                
        except Exception as e2:
            print(f"Manual collection also failed: {e2}")
            # ìµœì†Œí•œì˜ ë°ì´í„°ë¼ë„ ë°˜í™˜
            if not logger.episode_rewards:
                logger.episode_rewards = [0.0]
                logger.episode_successes = [0]
    
    return logger.episode_rewards, logger.episode_successes

def save(path: str, reward: list, success: list):
    """Save episode statistics to CSV."""
    try:
        # ë””ë ‰í† ë¦¬ ìƒì„±
        dir_path = os.path.dirname(path)
        if dir_path:  # ë¹ˆ ë¬¸ìì—´ì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ
            os.makedirs(dir_path, exist_ok=True)
        
        # ë°ì´í„°í”„ë ˆì„ ìƒì„± ë° ì €ì¥
        if reward and success:
            df = pd.DataFrame({
                "episode": np.arange(1, len(reward) + 1),
                "reward": reward,
                "success_rate": success
            })
            df.to_csv(path, index=False)
            print(f"âœ” {path} written with {len(reward)} episodes")
        else:
            print(f"âš  No data to save for {path}")
            
    except Exception as e:
        print(f"Error saving {path}: {e}")

def main():
    """Main training function."""
    p = argparse.ArgumentParser(description="Train PPO agents on dialogue environments")
    p.add_argument("--annotations", default="processed_annotations.json", 
                   help="Path to processed annotations file")
    p.add_argument("--steps", type=int, default=50_000,
                   help="Number of training steps")
    p.add_argument("--outdir", default="logs",
                   help="Output directory for logs")
    p.add_argument("--types", nargs='+', default=["human", "llm"],
                   help="Annotation types to train on")
    a = p.parse_args()
    
    print(f"Training configuration:")
    print(f"  Annotations: {a.annotations}")
    print(f"  Steps: {a.steps:,}")
    print(f"  Output dir: {a.outdir}")
    print(f"  Types: {a.types}")
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(a.outdir, exist_ok=True)
    
    results = {}
    
    for typ in a.types:
        print(f"\n{'='*50}")
        print(f"â–¶ Training on {typ} environment...")
        print(f"{'='*50}")
        
        try:
            # í™˜ê²½ ìƒì„±
            env = build_env(a.annotations, typ)
            
            # í•™ìŠµ ì‹¤í–‰
            rewards, successes = run(env, a.steps)
            
            # ê²°ê³¼ ì €ì¥
            output_path = os.path.join(a.outdir, f"{typ}_env.csv")
            save(output_path, rewards, successes)
            
            # ê²°ê³¼ ìš”ì•½
            if rewards:
                avg_reward = np.mean(rewards)
                success_rate = np.mean(successes) * 100
                print(f"ğŸ“Š {typ} Results:")
                print(f"   Episodes: {len(rewards)}")
                print(f"   Avg Reward: {avg_reward:.3f}")
                print(f"   Success Rate: {success_rate:.1f}%")
                
                results[typ] = {
                    'episodes': len(rewards),
                    'avg_reward': avg_reward,
                    'success_rate': success_rate
                }
            
        except Exception as e:
            print(f"âŒ Failed to train {typ} environment: {e}")
            results[typ] = {'error': str(e)}
    
    # ì „ì²´ ê²°ê³¼ ìš”ì•½
    print(f"\n{'='*50}")
    print("ğŸ Training Summary")
    print(f"{'='*50}")
    for typ, result in results.items():
        if 'error' in result:
            print(f"{typ}: Failed - {result['error']}")
        else:
            print(f"{typ}: {result['episodes']} episodes, "
                  f"avg reward {result['avg_reward']:.3f}, "
                  f"success rate {result['success_rate']:.1f}%")

if __name__ == "__main__":
    main()