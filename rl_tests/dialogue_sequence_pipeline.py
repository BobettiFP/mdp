#!/usr/bin/env python3
"""
Dialogue Sequence Analysis Pipeline
ê° dialogueì˜ state-action sequenceë¥¼ ë¶„ì„í•˜ê³  ìœ ì‚¬í•œ êµ¬ì¡°ì˜ dialogueë¥¼ ì°¾ìŒ
"""
import argparse, json, random, os
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
from collections import Counter, defaultdict
from typing import Dict, List, Tuple, Any, Optional
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.metrics import silhouette_score, pairwise_distances
from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering
from scipy.spatial.distance import pdist, squareform
from scipy.stats import entropy
from itertools import combinations
import warnings
warnings.filterwarnings('ignore')

# ===================== Sequence Representation Methods =====================

class SequenceEncoder:
    """ë‹¤ì–‘í•œ ë°©ë²•ìœ¼ë¡œ dialogue sequenceë¥¼ ë²¡í„°ë¡œ ì¸ì½”ë”©"""
    
    def __init__(self, method='combined'):
        self.method = method
        self.fitted = False
        
    def fit(self, sequences: List[List[Dict]], actions: List[List[str]] = None):
        """ëª¨ë“  sequencesë¥¼ ë³´ê³  vocabulary êµ¬ì¶•"""
        self.all_states = []
        self.all_actions = []
        
        for seq in sequences:
            for state in seq:
                self.all_states.append(state)
        
        if actions:
            for action_seq in actions:
                self.all_actions.extend(action_seq)
        
        # State vocabulary êµ¬ì¶•
        self.state_keys = sorted(set(key for state in self.all_states for key in state.keys()))
        self.state_values = sorted(set(str(val) for state in self.all_states for val in state.values()))
        
        # Action vocabulary êµ¬ì¶•
        if self.all_actions:
            self.action_vocab = sorted(set(self.all_actions))
        else:
            self.action_vocab = []
            
        self.fitted = True
        return self
    
    def _state_to_vector(self, state: Dict) -> np.ndarray:
        """ê°œë³„ stateë¥¼ ë²¡í„°ë¡œ ë³€í™˜"""
        # Slot presence + Value presence ë°©ì‹ (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
        vec = np.zeros(len(self.state_keys) + len(self.state_values), dtype=np.int8)
        
        for key in state:
            if key in self.state_keys:
                vec[self.state_keys.index(key)] = 1
                
        for val in state.values():
            val_str = str(val)
            if val_str in self.state_values:
                vec[len(self.state_keys) + self.state_values.index(val_str)] = 1
                
        return vec
    
    def _encode_statistical_features(self, sequence: List[Dict], actions: List[str] = None) -> np.ndarray:
        """í†µê³„ì  íŠ¹ì„±ìœ¼ë¡œ sequence ì¸ì½”ë”©"""
        features = []
        
        # ì‹œí€€ìŠ¤ ê¸¸ì´
        features.append(len(sequence))
        
        # ìœ ë‹ˆí¬ state ê°œìˆ˜
        unique_states = len(set(str(sorted(state.items())) for state in sequence))
        features.append(unique_states)
        
        # State diversity (entropy)
        state_counts = Counter(str(sorted(state.items())) for state in sequence)
        if len(state_counts) > 1:
            features.append(entropy(list(state_counts.values())))
        else:
            features.append(0.0)
            
        # State transition diversity
        if len(sequence) > 1:
            transitions = []
            for i in range(len(sequence) - 1):
                s1 = str(sorted(sequence[i].items()))
                s2 = str(sorted(sequence[i+1].items()))
                transitions.append(f"{s1}->{s2}")
            
            unique_transitions = len(set(transitions))
            features.append(unique_transitions)
            
            # Transition entropy
            trans_counts = Counter(transitions)
            features.append(entropy(list(trans_counts.values())))
        else:
            features.extend([0.0, 0.0])
            
        # State repetition ratio
        if len(sequence) > 0:
            features.append(unique_states / len(sequence))
        else:
            features.append(0.0)
            
        # Action diversity (if available)
        if actions:
            unique_actions = len(set(actions))
            features.append(unique_actions)
            
            if len(actions) > 1:
                action_counts = Counter(actions)
                features.append(entropy(list(action_counts.values())))
            else:
                features.append(0.0)
        else:
            features.extend([0.0, 0.0])
            
        return np.array(features, dtype=np.float32)
    
    def _encode_ngram_features(self, sequence: List[Dict], n=2) -> np.ndarray:
        """N-gram ê¸°ë°˜ sequence ì¸ì½”ë”©"""
        if len(sequence) < n:
            return np.zeros(100, dtype=np.float32)  # ê³ ì • í¬ê¸° ë²¡í„°
            
        # Stateë¥¼ stringìœ¼ë¡œ ë³€í™˜
        state_strings = [str(sorted(state.items())) for state in sequence]
        
        # N-gram ìƒì„±
        ngrams = []
        for i in range(len(state_strings) - n + 1):
            ngram = tuple(state_strings[i:i+n])
            ngrams.append(ngram)
            
        # N-gram frequency vector
        ngram_counts = Counter(ngrams)
        
        # ê³ ì • í¬ê¸° ë²¡í„°ë¡œ ë³€í™˜ (í•´ì‹± íŠ¸ë¦­ ì‚¬ìš©)
        vec = np.zeros(100, dtype=np.float32)
        for ngram, count in ngram_counts.items():
            hash_val = hash(str(ngram)) % 100
            vec[hash_val] += count
            
        # ì •ê·œí™”
        if np.sum(vec) > 0:
            vec = vec / np.sum(vec)
            
        return vec
    
    def _encode_sequence_embedding(self, sequence: List[Dict]) -> np.ndarray:
        """Sequenceë¥¼ ê³ ì • í¬ê¸° embeddingìœ¼ë¡œ ë³€í™˜"""
        if not sequence:
            return np.zeros(self.get_embedding_dim())
            
        # ê° stateë¥¼ ë²¡í„°ë¡œ ë³€í™˜
        state_vectors = [self._state_to_vector(state) for state in sequence]
        state_matrix = np.array(state_vectors)
        
        # ë‹¤ì–‘í•œ aggregation ë°©ë²•ë“¤
        embeddings = []
        
        # 1. Mean pooling
        embeddings.append(np.mean(state_matrix, axis=0))
        
        # 2. Max pooling
        embeddings.append(np.max(state_matrix, axis=0))
        
        # 3. First and last states
        embeddings.append(state_matrix[0])
        embeddings.append(state_matrix[-1])
        
        # 4. Std pooling (variation)
        if len(state_matrix) > 1:
            embeddings.append(np.std(state_matrix, axis=0))
        else:
            embeddings.append(np.zeros(state_matrix.shape[1]))
            
        return np.concatenate(embeddings)
    
    def get_embedding_dim(self) -> int:
        """ì„ë² ë”© ì°¨ì› ê³„ì‚°"""
        if not self.fitted:
            raise ValueError("Encoder must be fitted first")
            
        state_dim = len(self.state_keys) + len(self.state_values)
        
        if self.method == 'statistical':
            return 8  # í†µê³„ì  íŠ¹ì„± ê°œìˆ˜
        elif self.method == 'ngram':
            return 100  # ê³ ì • í¬ê¸°
        elif self.method == 'embedding':
            return state_dim * 5  # mean, max, first, last, std
        elif self.method == 'combined':
            return 8 + 100 + state_dim * 5  # ëª¨ë“  ë°©ë²• ê²°í•©
        else:
            raise ValueError(f"Unknown method: {self.method}")
    
    def transform(self, sequences: List[List[Dict]], actions: List[List[str]] = None) -> np.ndarray:
        """Sequencesë¥¼ ë²¡í„°ë“¤ë¡œ ë³€í™˜"""
        if not self.fitted:
            raise ValueError("Encoder must be fitted first")
            
        encoded_sequences = []
        
        for i, sequence in enumerate(sequences):
            action_seq = actions[i] if actions else None
            
            if self.method == 'statistical':
                enc = self._encode_statistical_features(sequence, action_seq)
            elif self.method == 'ngram':
                enc = self._encode_ngram_features(sequence)
            elif self.method == 'embedding':
                enc = self._encode_sequence_embedding(sequence)
            elif self.method == 'combined':
                stat_enc = self._encode_statistical_features(sequence, action_seq)
                ngram_enc = self._encode_ngram_features(sequence)
                emb_enc = self._encode_sequence_embedding(sequence)
                enc = np.concatenate([stat_enc, ngram_enc, emb_enc])
            else:
                raise ValueError(f"Unknown method: {self.method}")
                
            encoded_sequences.append(enc)
            
        return np.array(encoded_sequences)

# ===================== Sequence Distance Metrics =====================

def dtw_distance(seq1: List[Dict], seq2: List[Dict], state_encoder) -> float:
    """Dynamic Time Warping distance between sequences"""
    # State sequencesë¥¼ ë²¡í„°ë¡œ ë³€í™˜
    vec1 = np.array([state_encoder._state_to_vector(s) for s in seq1])
    vec2 = np.array([state_encoder._state_to_vector(s) for s in seq2])
    
    n, m = len(vec1), len(vec2)
    
    # DTW matrix ì´ˆê¸°í™”
    dtw_matrix = np.full((n + 1, m + 1), np.inf)
    dtw_matrix[0, 0] = 0
    
    # DTW ê³„ì‚°
    for i in range(1, n + 1):
        for j in range(1, m + 1):
            cost = np.linalg.norm(vec1[i-1] - vec2[j-1])
            dtw_matrix[i, j] = cost + min(
                dtw_matrix[i-1, j],      # insertion
                dtw_matrix[i, j-1],      # deletion
                dtw_matrix[i-1, j-1]     # match
            )
    
    return dtw_matrix[n, m] / max(n, m)  # ì •ê·œí™”

def edit_distance(seq1: List[Dict], seq2: List[Dict]) -> float:
    """Edit distance between state sequences"""
    # Stateë¥¼ stringìœ¼ë¡œ ë³€í™˜
    str1 = [str(sorted(s.items())) for s in seq1]
    str2 = [str(sorted(s.items())) for s in seq2]
    
    n, m = len(str1), len(str2)
    dp = [[0] * (m + 1) for _ in range(n + 1)]
    
    # ì´ˆê¸°í™”
    for i in range(n + 1):
        dp[i][0] = i
    for j in range(m + 1):
        dp[0][j] = j
    
    # DP ê³„ì‚°
    for i in range(1, n + 1):
        for j in range(1, m + 1):
            if str1[i-1] == str2[j-1]:
                dp[i][j] = dp[i-1][j-1]
            else:
                dp[i][j] = 1 + min(dp[i-1][j], dp[i][j-1], dp[i-1][j-1])
    
    return dp[n][m] / max(n, m)  # ì •ê·œí™”

# ===================== Main Analysis Functions =====================

def extract_dialogue_sequences(data: List[Dict]) -> Tuple[List[List[Dict]], List[List[str]], List[str]]:
    """ë°ì´í„°ì—ì„œ dialogueë³„ sequence ì¶”ì¶œ"""
    dialogues = defaultdict(list)
    dialogue_ids = []
    
    # annotation_typeë³„ë¡œ ë‹¤ë¥¸ ì²˜ë¦¬
    human_data = [r for r in data if r.get('annotation_type') == 'human']
    llm_data = [r for r in data if r.get('annotation_type') == 'llm']
    
    # Human ë°ì´í„° ì²˜ë¦¬ (ê¸°ì¡´ ë°©ì‹)
    for record in human_data:
        dialogue_id = record.get('dialogue_id', record.get('id', 'unknown'))
        dialogues[f"human_{dialogue_id}"].append(record)
    
    # LLM ë°ì´í„° ì²˜ë¦¬ (íŠ¹ë³„í•œ ë¡œì§ ì ìš©)
    llm_dialogues = group_llm_dialogues(llm_data)
    for dialogue_id, records in llm_dialogues.items():
        dialogues[f"llm_{dialogue_id}"] = records
    
    sequences = []
    action_sequences = []
    
    for dialogue_id, records in dialogues.items():
        # Turn ìˆœì„œë¡œ ì •ë ¬ (turn_id ìš°ì„ , ì—†ìœ¼ë©´ turn, step ìˆœ)
        records.sort(key=lambda x: x.get('turn_id', x.get('turn', x.get('step', 0))))
        
        # State sequence ì¶”ì¶œ
        state_seq = [r['state_after'] for r in records if 'state_after' in r]
        action_seq = [r.get('action', r.get('annotation', '')) for r in records]
        
        if state_seq:  # ë¹ˆ ì‹œí€€ìŠ¤ ì œì™¸
            sequences.append(state_seq)
            action_sequences.append(action_seq)
            dialogue_ids.append(dialogue_id)
    
    return sequences, action_sequences, dialogue_ids

def group_llm_dialogues(llm_data: List[Dict]) -> Dict[str, List[Dict]]:
    """LLM ë°ì´í„°ë¥¼ dialogueë¡œ ê·¸ë£¹í™”í•˜ëŠ” íŠ¹ë³„í•œ ë¡œì§"""
    dialogues = defaultdict(list)
    
    # ë°©ë²• 1: state transitionì„ ê¸°ë°˜ìœ¼ë¡œ dialogue êµ¬ë¶„
    current_dialogue_id = 0
    current_dialogue = []
    
    for i, record in enumerate(llm_data):
        current_dialogue.append(record)
        
        # ìƒˆë¡œìš´ dialogue ì‹œì‘ ì¡°ê±´ë“¤ì„ ì²´í¬
        is_new_dialogue = False
        
        # ì¡°ê±´ 1: state_beforeê°€ ë¹„ì–´ìˆê³  ì´ì „ì— ë°ì´í„°ê°€ ìˆì—ˆë‹¤ë©´
        if (record.get('state_before', {}) == {} and 
            len(current_dialogue) > 1):
            is_new_dialogue = True
        
        # ì¡°ê±´ 2: ì´ì „ recordì˜ state_afterì™€ í˜„ì¬ recordì˜ state_beforeê°€ ì™„ì „íˆ ë‹¤ë¥´ë‹¤ë©´
        elif (i > 0 and 
              current_dialogue and len(current_dialogue) > 1):
            prev_record = llm_data[i-1]
            prev_state = prev_record.get('state_after', {})
            curr_state_before = record.get('state_before', {})
            
            # ë„ë©”ì¸ì´ ì™„ì „íˆ ë°”ë€Œì—ˆëŠ”ì§€ ì²´í¬
            prev_domains = set(key.split('_')[0] for key in prev_state.keys())
            curr_domains = set(key.split('_')[0] for key in curr_state_before.keys())
            
            if (prev_domains and curr_domains and 
                not prev_domains.intersection(curr_domains)):
                is_new_dialogue = True
        
        # ì¡°ê±´ 3: ì¼ì • ê¸¸ì´ë§ˆë‹¤ ê°•ì œë¡œ dialogue êµ¬ë¶„ (ë„ˆë¬´ ê¸´ dialogue ë°©ì§€)
        elif len(current_dialogue) > 20:  # 20í„´ ë„˜ìœ¼ë©´ ìƒˆ dialogue
            is_new_dialogue = True
            
        # ìƒˆ dialogue ì‹œì‘
        if is_new_dialogue and len(current_dialogue) > 1:
            # í˜„ì¬ recordëŠ” ìƒˆ dialogueì— í¬í•¨
            dialogues[f"auto_{current_dialogue_id}"] = current_dialogue[:-1]
            current_dialogue = [record]
            current_dialogue_id += 1
    
    # ë§ˆì§€ë§‰ dialogue ì¶”ê°€
    if current_dialogue:
        dialogues[f"auto_{current_dialogue_id}"] = current_dialogue
    
    # ê° dialogueì˜ ê¸¸ì´ê°€ 1ì¸ ê²½ìš°ë“¤ì„ ë³„ë„ ì²˜ë¦¬
    single_turn_dialogues = {}
    multi_turn_dialogues = {}
    
    for dialogue_id, records in dialogues.items():
        if len(records) == 1:
            # ë‹¨ì¼ í„´ì€ ê°œë³„ dialogueë¡œ ì²˜ë¦¬
            single_turn_dialogues[f"single_{dialogue_id}"] = records
        else:
            multi_turn_dialogues[dialogue_id] = records
    
    # ê²°ê³¼ í•©ì¹˜ê¸°
    result = {**multi_turn_dialogues, **single_turn_dialogues}
    
    print(f"   â”” LLM dialogue ìë™ êµ¬ë¶„: {len(result)}ê°œ dialogues ìƒì„±")
    print(f"     - Multi-turn: {len(multi_turn_dialogues)}ê°œ")
    print(f"     - Single-turn: {len(single_turn_dialogues)}ê°œ")
    
    return result

def compute_sequence_similarities(sequences: List[List[Dict]], 
                                method: str = 'combined',
                                distance_metric: str = 'euclidean') -> np.ndarray:
    """Sequence ê°„ ìœ ì‚¬ë„ í–‰ë ¬ ê³„ì‚°"""
    
    # Sequence encoder ì´ˆê¸°í™” ë° í•™ìŠµ
    encoder = SequenceEncoder(method=method)
    encoder.fit(sequences)
    
    # Sequencesë¥¼ ë²¡í„°ë¡œ ë³€í™˜
    if method in ['statistical', 'ngram', 'embedding', 'combined']:
        X = encoder.transform(sequences)
        # ê±°ë¦¬ í–‰ë ¬ ê³„ì‚°
        distances = pairwise_distances(X, metric=distance_metric)
        
    elif method == 'dtw':
        # DTW distance ê³„ì‚°
        n = len(sequences)
        distances = np.zeros((n, n))
        
        for i in range(n):
            for j in range(i+1, n):
                dist = dtw_distance(sequences[i], sequences[j], encoder)
                distances[i, j] = distances[j, i] = dist
                
    elif method == 'edit':
        # Edit distance ê³„ì‚°
        n = len(sequences)
        distances = np.zeros((n, n))
        
        for i in range(n):
            for j in range(i+1, n):
                dist = edit_distance(sequences[i], sequences[j])
                distances[i, j] = distances[j, i] = dist
    
    # Distanceë¥¼ similarityë¡œ ë³€í™˜ (ì˜µì…˜)
    # similarities = 1 / (1 + distances)
    
    return distances, X if method in ['statistical', 'ngram', 'embedding', 'combined'] else None

def find_optimal_clusters_for_sequences(X: np.ndarray, max_clusters: int = None) -> int:
    """Sequence clusteringì„ ìœ„í•œ ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ ê²°ì •"""
    if len(X) < 3:
        return 1
    
    max_k = max_clusters or min(max(len(X) // 5, 3), len(X) - 1)
    max_k = max(2, min(max_k, len(X) - 1))
    
    if max_k < 2:
        return 1
    
    K_range = range(2, max_k + 1)
    silhouette_scores = []
    inertias = []
    
    for k in K_range:
        kmeans = KMeans(n_clusters=k, n_init=10, random_state=42)
        labels = kmeans.fit_predict(X)
        
        if len(np.unique(labels)) > 1:
            sil_score = silhouette_score(X, labels)
            silhouette_scores.append(sil_score)
            inertias.append(kmeans.inertia_)
        else:
            silhouette_scores.append(-1)
            inertias.append(float('inf'))
    
    if not silhouette_scores or max(silhouette_scores) < 0:
        return 2
    
    # ìµœê³  ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´ë¥¼ ê°€ì§„ í´ëŸ¬ìŠ¤í„° ìˆ˜ ì„ íƒ
    optimal_k = K_range[np.argmax(silhouette_scores)]
    return optimal_k

def cluster_dialogues(X: np.ndarray, dialogue_ids: List[str], optimal_k: int) -> Dict:
    """Dialogue sequence clustering"""
    if len(X) < 2:
        return {
            'labels': np.zeros(len(X)),
            'cluster_info': {},
            'n_clusters': 1 if len(X) > 0 else 0,
            'method': 'single_dialogue'
        }
    
    # K-means clustering
    kmeans = KMeans(n_clusters=optimal_k, n_init=10, random_state=42)
    labels = kmeans.fit_predict(X)
    
    # ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´ ê³„ì‚°
    silhouette = silhouette_score(X, labels) if len(np.unique(labels)) > 1 else -1
    
    # í´ëŸ¬ìŠ¤í„° ì •ë³´ êµ¬ì„±
    cluster_info = {}
    for cluster_id in np.unique(labels):
        cluster_mask = labels == cluster_id
        cluster_dialogues = [dialogue_ids[i] for i in np.where(cluster_mask)[0]]
        cluster_points = X[cluster_mask]
        
        # í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ì— ê°€ì¥ ê°€ê¹Œìš´ dialogue ì°¾ê¸°
        center = kmeans.cluster_centers_[cluster_id]
        distances = np.linalg.norm(cluster_points - center, axis=1)
        representative_idx = np.argmin(distances)
        representative_dialogue = cluster_dialogues[representative_idx]
        
        cluster_info[f'cluster_{cluster_id}'] = {
            'size': int(np.sum(cluster_mask)),
            'dialogues': cluster_dialogues,
            'representative_dialogue': representative_dialogue,
            'avg_distance_to_center': float(np.mean(distances)),
            'intra_cluster_diversity': float(np.std(distances))
        }
    
    return {
        'labels': labels,
        'cluster_info': cluster_info,
        'n_clusters': len(np.unique(labels)),
        'centroids': kmeans.cluster_centers_,
        'method': 'kmeans',
        'silhouette_score': silhouette
    }

def visualize_dialogue_space(X: np.ndarray, labels: np.ndarray, dialogue_ids: List[str], 
                           title: str, output_path: Path):
    """Dialogue space t-SNE ì‹œê°í™”"""
    if len(X) < 3:
        return
    
    try:
        # t-SNE ì„ë² ë”©
        perplexity = min(30, len(X) // 3)
        if perplexity < 2:
            perplexity = 2
        
        tsne = TSNE(n_components=2, random_state=42, perplexity=perplexity)
        embedding = tsne.fit_transform(X)
    except:
        # t-SNE ì‹¤íŒ¨ì‹œ PCA ì‚¬ìš©
        pca = PCA(n_components=2)
        embedding = pca.fit_transform(X)
    
    plt.figure(figsize=(12, 8))
    
    # í´ëŸ¬ìŠ¤í„°ë³„ë¡œ ë‹¤ë¥¸ ìƒ‰ìƒê³¼ ë§ˆì»¤ ì‚¬ìš©
    unique_labels = np.unique(labels)
    colors = plt.cm.tab10(np.linspace(0, 1, len(unique_labels)))
    
    for i, cluster_id in enumerate(unique_labels):
        cluster_mask = labels == cluster_id
        cluster_points = embedding[cluster_mask]
        
        plt.scatter(cluster_points[:, 0], cluster_points[:, 1], 
                   c=[colors[i]], label=f'Cluster {cluster_id}', 
                   s=50, alpha=0.7)
        
        # ëª‡ ê°œ dialogue ID í‘œì‹œ (ë„ˆë¬´ ë§ìœ¼ë©´ ìƒëµ)
        cluster_indices = np.where(cluster_mask)[0]
        if len(cluster_indices) <= 10:  # 10ê°œ ì´í•˜ë§Œ ë¼ë²¨ í‘œì‹œ
            for idx in cluster_indices:
                plt.annotate(dialogue_ids[idx], 
                           (embedding[idx, 0], embedding[idx, 1]),
                           xytext=(5, 5), textcoords='offset points',
                           fontsize=8, alpha=0.7)
    
    plt.xlabel('Dimension 1')
    plt.ylabel('Dimension 2')
    plt.title(f'{title} - Dialogue Sequence Clustering')
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.tight_layout()
    plt.savefig(output_path, dpi=300, bbox_inches='tight')
    plt.close()

# ===================== Main Function =====================

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--annotations", required=True, help="JSON file with dialogue annotations")
    parser.add_argument("--outdir", default="dialogue_sequence_results", help="Output directory")
    parser.add_argument("--encoding_method", default="combined", 
                       choices=['statistical', 'ngram', 'embedding', 'combined', 'dtw', 'edit'],
                       help="Sequence encoding method")
    parser.add_argument("--distance_metric", default="euclidean",
                       choices=['euclidean', 'cosine', 'manhattan'],
                       help="Distance metric for clustering")
    parser.add_argument("--max_clusters", type=int, default=None,
                       help="Maximum number of clusters")
    
    args = parser.parse_args()
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir = Path(args.outdir)
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # ë°ì´í„° ë¡œë“œ
    with open(args.annotations, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    if isinstance(data, dict):
        records = data.get("annotations", [])
    else:
        records = data
    
    # Humanê³¼ LLM ë°ì´í„° ë¶„ë¦¬
    human_records = [r for r in records if r.get("annotation_type") == "human"]
    llm_records = [r for r in records if r.get("annotation_type") == "llm"]
    
    if not human_records or not llm_records:
        raise ValueError("annotation_typeì´ 'human' ë˜ëŠ” 'llm'ì¸ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤.")
    
    print(f"ğŸ“Š ë°ì´í„° ë¡œë“œ ì™„ë£Œ: Human {len(human_records)}, LLM {len(llm_records)}")
    
    # Dialogue sequence ì¶”ì¶œ
    print("ğŸ” Dialogue sequences ì¶”ì¶œ ì¤‘...")
    all_sequences, all_actions, all_dialogue_ids = extract_dialogue_sequences(records)
    
    # Humanê³¼ LLM sequences ë¶„ë¦¬
    human_sequences, human_actions, human_dialogue_ids = [], [], []
    llm_sequences, llm_actions, llm_dialogue_ids = [], [], []
    
    for seq, acts, did in zip(all_sequences, all_actions, all_dialogue_ids):
        if did.startswith('human_'):
            human_sequences.append(seq)
            human_actions.append(acts)
            human_dialogue_ids.append(did)
        elif did.startswith('llm_'):
            llm_sequences.append(seq)
            llm_actions.append(acts)
            llm_dialogue_ids.append(did)
    
    print(f"   Human dialogues: {len(human_sequences)}")
    print(f"   LLM dialogues: {len(llm_sequences)}")
    
    # ë¶„ì„ ë° í´ëŸ¬ìŠ¤í„°ë§
    results = {}
    
    for data_type, sequences, dialogue_ids in [
        ("human", human_sequences, human_dialogue_ids),
        ("llm", llm_sequences, llm_dialogue_ids)
    ]:
        print(f"\nğŸ” {data_type.upper()} sequences ë¶„ì„ ì¤‘...")
        
        if len(sequences) < 2:
            print(f"   âš ï¸  {data_type} dialogue ìˆ˜ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤ (ìµœì†Œ 2ê°œ í•„ìš”)")
            continue
        
        # ìœ ì‚¬ë„ ê³„ì‚°
        distances, X = compute_sequence_similarities(
            sequences, 
            method=args.encoding_method,
            distance_metric=args.distance_metric
        )
        
        if X is not None:
            # í´ëŸ¬ìŠ¤í„°ë§
            optimal_k = find_optimal_clusters_for_sequences(X, args.max_clusters)
            clustering_result = cluster_dialogues(X, dialogue_ids, optimal_k)
            
            print(f"   â”” í´ëŸ¬ìŠ¤í„° ìˆ˜: {clustering_result['n_clusters']}, "
                  f"ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´: {clustering_result['silhouette_score']:.3f}")
            
            # ì‹œê°í™”
            visualize_dialogue_space(
                X, clustering_result['labels'], dialogue_ids,
                data_type.upper(), output_dir / f"dialogue_space_{data_type}.png"
            )
            
            # ê²°ê³¼ ì €ì¥
            results[data_type] = {
                'n_dialogues': len(sequences),
                'encoding_method': args.encoding_method,
                'clustering': {
                    'method': clustering_result['method'],
                    'n_clusters': clustering_result['n_clusters'],
                    'silhouette_score': clustering_result['silhouette_score'],
                    'cluster_info': clustering_result['cluster_info']
                }
            }
            
            # í´ëŸ¬ìŠ¤í„° ì •ë³´ ì¶œë ¥
            print(f"\nğŸ“Š {data_type.upper()} í´ëŸ¬ìŠ¤í„° ë¶„ì„:")
            for cluster_name, info in clustering_result['cluster_info'].items():
                print(f"  {cluster_name}: {info['size']}ê°œ dialogues")
                print(f"    ëŒ€í‘œ dialogue: {info['representative_dialogue']}")
                print(f"    ë‹¤ì–‘ì„±: {info['intra_cluster_diversity']:.3f}")
    
    # ê²°ê³¼ ì €ì¥
    output_file = output_dir / "dialogue_sequence_analysis.json"
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ… ë¶„ì„ ì™„ë£Œ. ê²°ê³¼ ì €ì¥: {output_file}")

if __name__ == "__main__":
    np.random.seed(42)
    random.seed(42)
    main()