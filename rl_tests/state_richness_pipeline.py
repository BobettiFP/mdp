#!/usr/bin/env python3
"""
ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ ìƒíƒœ ê³µê°„ í’ë¶€í•¨ â†’ ì¼ë°˜í™” ëŠ¥ë ¥ ì¦ëª… íŒŒì´í”„ë¼ì¸
================================================================
mdp/processed_annotations.json ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬
LLM annotationì´ ë” í’ë¶€í•œ ìƒíƒœ ê³µê°„ì„ ì œê³µí•˜ì—¬ 
ë” ë‚˜ì€ ì¼ë°˜í™”ë¥¼ ë‹¬ì„±í•œë‹¤ëŠ” ê°€ì„¤ì„ ê²€ì¦
"""

import numpy as np
import torch
import torch.nn as nn
from sklearn.manifold import TSNE
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from sklearn.decomposition import PCA
from sklearn.neighbors import NearestNeighbors
from scipy.spatial.distance import pdist, squareform
from scipy.stats import entropy, ks_2samp
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Set, Union, Any
from dataclasses import dataclass
import json
from pathlib import Path
import os
from collections import defaultdict, Counter

# ê²°ê³¼ ì €ì¥ ê²½ë¡œëŠ” ì‹¤í–‰ ì‹œì— ì„¤ì •

@dataclass
class StateRichnessMetrics:
    """ìƒíƒœ ê³µê°„ í’ë¶€í•¨ ë©”íŠ¸ë¦­"""
    # 1. ë‹¤ì–‘ì„± ë©”íŠ¸ë¦­
    unique_states: int
    state_entropy: float
    effective_dimensionality: float
    
    # 2. ë¶„í¬ ë©”íŠ¸ë¦­  
    coverage_ratio: float
    density_uniformity: float
    cluster_separation: float
    
    # 3. êµ¬ì¡°ì  ë©”íŠ¸ë¦­
    intrinsic_dimensionality: int
    manifold_complexity: float
    transition_diversity: float
    
    # 4. ì–´íœ˜ í’ë¶€í•¨
    slot_vocabulary_size: int
    value_vocabulary_size: int
    slot_value_combinations: int

@dataclass 
class GeneralizationMetrics:
    """ì¼ë°˜í™” ëŠ¥ë ¥ ë©”íŠ¸ë¦­"""
    # 1. ë„ë©”ì¸ ì „ì´ ì„±ëŠ¥
    cross_domain_performance: Dict[str, float]
    domain_adaptation_speed: Dict[str, int]
    
    # 2. Few-shot ì„±ëŠ¥
    few_shot_accuracy: Dict[int, float]  # {shots: accuracy}
    sample_efficiency: float
    
    # 3. ê²¬ê³ ì„±
    noise_robustness: float
    distribution_shift_robustness: float
    unseen_combination_handling: float

class RealDataStateSpaceAnalyzer:
    """ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ ìƒíƒœ ê³µê°„ ë¶„ì„ê¸°"""
    
    def __init__(self, annotations: List[dict], annotation_type: str, canonical_map: Dict[str, str]):
        self.annotations = annotations
        self.annotation_type = annotation_type
        self.canonical_map = canonical_map
        self.state_vectors = None
        self.state_embeddings = None
        self.slot_vocab = {}
        self.value_vocab = {}
        
        print(f"[{self.annotation_type}] ì´ˆê¸°í™”: {len(annotations)}ê°œ annotation")
        
    def _normalize_slot_name(self, slot_name: str) -> str:
        """ìŠ¬ë¡¯ ì´ë¦„ ì •ê·œí™” (canonical_map í™œìš©)"""
        # canonical_mapì— ì •ì˜ëœ ë§¤í•‘ ì‚¬ìš©
        if slot_name in self.canonical_map:
            return self.canonical_map[slot_name]
        
        # ê¸°ë³¸ ì •ê·œí™” (ì†Œë¬¸ì, ì–¸ë”ìŠ¤ì½”ì–´)
        normalized = slot_name.lower().replace("-", "_")
        
        # ë„ë©”ì¸ ì ‘ë‘ì‚¬ ì œê±°
        domain_prefixes = ["restaurant_", "hotel_", "train_", "taxi_", "attraction_", 
                          "bus_", "hospital_", "police_"]
        for prefix in domain_prefixes:
            if normalized.startswith(prefix):
                normalized = normalized[len(prefix):]
                break
        
        return normalized
    
    def _extract_slot_value(self, slot_data: Union[str, Dict]) -> str:
        """ìŠ¬ë¡¯ ê°’ ì¶”ì¶œ (ë³µì¡í•œ êµ¬ì¡° ì²˜ë¦¬)"""
        if isinstance(slot_data, str):
            return slot_data
        elif isinstance(slot_data, dict):
            # {"value": "<time_any>", "slot_type": "time_any"} í˜•íƒœ
            return str(slot_data.get("value", str(slot_data)))
        elif isinstance(slot_data, list):
            # ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ì²« ë²ˆì§¸ ê°’ ì‚¬ìš©
            return str(slot_data[0]) if slot_data else ""
        elif isinstance(slot_data, (int, float)):
            # ìˆ«ìì¸ ê²½ìš° ë¬¸ìì—´ë¡œ ë³€í™˜
            return str(slot_data)
        else:
            return str(slot_data)
    
    def extract_state_vectors(self) -> np.ndarray:
        """ì‹¤ì œ ë°ì´í„°ì—ì„œ ìƒíƒœ ë²¡í„° ì¶”ì¶œ"""
        print(f"[{self.annotation_type}] ìƒíƒœ ë²¡í„° ì¶”ì¶œ ì¤‘...")
        
        # ëª¨ë“  ì •ê·œí™”ëœ ìŠ¬ë¡¯ê³¼ ê°’ ìˆ˜ì§‘
        all_normalized_slots = set()
        all_values = set()
        
        valid_annotations = []
        
        for ann in self.annotations:
            if not ann.get('slots'):
                continue
                
            normalized_slots = {}
            for slot_name, slot_value in ann['slots'].items():
                normalized_slot = self._normalize_slot_name(slot_name)
                extracted_value = self._extract_slot_value(slot_value)
                
                normalized_slots[normalized_slot] = extracted_value
                all_normalized_slots.add(normalized_slot)
                all_values.add(extracted_value)  # ì´ë¯¸ _extract_slot_valueì—ì„œ str()ë¡œ ë³€í™˜ë¨
            
            if normalized_slots:  # ìœ íš¨í•œ ìŠ¬ë¡¯ì´ ìˆëŠ” ê²½ìš°ë§Œ
                ann_copy = ann.copy()
                ann_copy['normalized_slots'] = normalized_slots
                valid_annotations.append(ann_copy)
        
        self.annotations = valid_annotations
        print(f"ìœ íš¨í•œ annotation ìˆ˜: {len(self.annotations)}")
        
        # Vocabulary êµ¬ì¶• (ëª¨ë“  ê°’ì´ ë¬¸ìì—´ì„ì„ í™•ì¸)
        try:
            # ì•ˆì „í•œ ì •ë ¬ì„ ìœ„í•´ ëª¨ë“  ê°’ì„ ë¬¸ìì—´ë¡œ í™•ì‹¤íˆ ë³€í™˜
            safe_values = {str(v) for v in all_values}
            
            self.slot_vocab = {slot: i for i, slot in enumerate(sorted(all_normalized_slots))}
            self.value_vocab = {value: i for i, value in enumerate(sorted(safe_values))}
        except Exception as e:
            print(f"Vocabulary êµ¬ì¶• ì¤‘ ì˜¤ë¥˜: {e}")
            print(f"ìŠ¬ë¡¯ íƒ€ì…ë“¤: {set(type(s).__name__ for s in all_normalized_slots)}")
            print(f"ê°’ íƒ€ì…ë“¤: {set(type(v).__name__ for v in all_values)}")
            print(f"ê°’ ìƒ˜í”Œ: {list(all_values)[:10]}")
            raise
        
        print(f"ìŠ¬ë¡¯ vocabulary í¬ê¸°: {len(self.slot_vocab)}")
        print(f"ê°’ vocabulary í¬ê¸°: {len(self.value_vocab)}")
        
        # ë””ë²„ê¹…: ê°’ íƒ€ì… í™•ì¸ (ë¬¸ìì—´ë¡œ ë³€í™˜ëœ í›„)
        safe_values = {str(v) for v in all_values}
        value_types = set(type(v).__name__ for v in safe_values)
        print(f"ê°’ íƒ€ì…ë“¤: {value_types}")
        
        # ìƒíƒœ ë²¡í„° ìƒì„±
        vectors = []
        for ann in self.annotations:
            # ìŠ¬ë¡¯ ì¡´ì¬ ì—¬ë¶€ ë²¡í„°
            slot_vec = np.zeros(len(self.slot_vocab))
            for slot in ann['normalized_slots']:
                if slot in self.slot_vocab:
                    slot_vec[self.slot_vocab[slot]] = 1
            
            # ê°’ ì¡´ì¬ ì—¬ë¶€ ë²¡í„°  
            value_vec = np.zeros(len(self.value_vocab))
            for value in ann['normalized_slots'].values():
                value_str = str(value)  # í™•ì‹¤íˆ ë¬¸ìì—´ë¡œ ë³€í™˜
                if value_str in self.value_vocab:
                    value_vec[self.value_vocab[value_str]] = 1
            
            # ê²°í•©ëœ ìƒíƒœ ë²¡í„°
            state_vector = np.concatenate([slot_vec, value_vec])
            vectors.append(state_vector)
        
        self.state_vectors = np.array(vectors)
        print(f"ìƒíƒœ ë²¡í„° í˜•íƒœ: {self.state_vectors.shape}")
        return self.state_vectors
    
    def calculate_richness_metrics(self) -> StateRichnessMetrics:
        """ìƒíƒœ ê³µê°„ í’ë¶€í•¨ ë©”íŠ¸ë¦­ ê³„ì‚°"""
        print(f"[{self.annotation_type}] í’ë¶€í•¨ ë©”íŠ¸ë¦­ ê³„ì‚° ì¤‘...")
        
        if self.state_vectors is None:
            self.extract_state_vectors()
        
        if len(self.state_vectors) == 0:
            print(f"ê²½ê³ : {self.annotation_type}ì— ìœ íš¨í•œ ìƒíƒœ ë²¡í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return StateRichnessMetrics(
                unique_states=0, state_entropy=0.0, effective_dimensionality=0,
                coverage_ratio=0.0, density_uniformity=0.0, cluster_separation=0.0,
                intrinsic_dimensionality=0, manifold_complexity=0.0, transition_diversity=0.0,
                slot_vocabulary_size=0, value_vocabulary_size=0, slot_value_combinations=0
            )
        
        # 1. ë‹¤ì–‘ì„± ë©”íŠ¸ë¦­
        unique_states = len(np.unique(self.state_vectors, axis=0))
        
        # ìƒíƒœ ë¶„í¬ ì—”íŠ¸ë¡œí”¼
        state_strings = [str(vec.tolist()) for vec in self.state_vectors]
        state_counts = Counter(state_strings)
        state_probs = np.array(list(state_counts.values())) / len(state_strings)
        state_entropy = entropy(state_probs)
        
        # ìœ íš¨ ì°¨ì›ìˆ˜ (PCA ê¸°ë°˜)
        try:
            pca = PCA()
            pca.fit(self.state_vectors)
            cumulative_var = np.cumsum(pca.explained_variance_ratio_)
            effective_dim = np.argmax(cumulative_var >= 0.95) + 1
        except:
            effective_dim = min(10, self.state_vectors.shape[1])
        
        # 2. ë¶„í¬ ë©”íŠ¸ë¦­
        coverage_ratio = unique_states / len(self.state_vectors)
        
        # ë°€ë„ ê· ì¼ì„± (í´ëŸ¬ìŠ¤í„°ë§ ê¸°ë°˜)
        try:
            if unique_states > 10:
                n_clusters = min(10, unique_states // 2)
                kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
                cluster_labels = kmeans.fit_predict(self.state_vectors)
                density_uniformity = silhouette_score(self.state_vectors, cluster_labels)
            else:
                density_uniformity = 0.5  # ê¸°ë³¸ê°’
        except:
            density_uniformity = 0.0
        
        # í´ëŸ¬ìŠ¤í„° ë¶„ë¦¬ë„
        try:
            if unique_states > 1:
                unique_vectors = np.unique(self.state_vectors, axis=0)
                distances = pdist(unique_vectors)
                cluster_separation = np.mean(distances)
            else:
                cluster_separation = 0.0
        except:
            cluster_separation = 0.0
        
        # 3. êµ¬ì¡°ì  ë©”íŠ¸ë¦­  
        intrinsic_dim = min(effective_dim, self.state_vectors.shape[1])
        
        # ë§¤ë‹ˆí´ë“œ ë³µì¡ì„±
        try:
            if len(self.state_vectors) > 5:
                k = min(5, len(self.state_vectors) - 1)
                nn = NearestNeighbors(n_neighbors=k)
                nn.fit(self.state_vectors)
                distances, _ = nn.kneighbors(self.state_vectors)
                manifold_complexity = np.std(distances.mean(axis=1))
            else:
                manifold_complexity = 0.0
        except:
            manifold_complexity = 0.0
        
        # ì „ì´ ë‹¤ì–‘ì„± (ëŒ€í™”ë³„ ìƒíƒœ ë³€í™”)
        transition_diversity = self._calculate_transition_diversity()
        
        # 4. ì–´íœ˜ í’ë¶€í•¨
        slot_combinations = set()
        for ann in self.annotations:
            slots_tuple = tuple(sorted(ann['normalized_slots'].keys()))
            slot_combinations.add(slots_tuple)
        
        return StateRichnessMetrics(
            unique_states=unique_states,
            state_entropy=state_entropy,
            effective_dimensionality=effective_dim,
            coverage_ratio=coverage_ratio,
            density_uniformity=density_uniformity,
            cluster_separation=cluster_separation,
            intrinsic_dimensionality=intrinsic_dim,
            manifold_complexity=manifold_complexity,
            transition_diversity=transition_diversity,
            slot_vocabulary_size=len(self.slot_vocab),
            value_vocabulary_size=len(self.value_vocab),
            slot_value_combinations=len(slot_combinations)
        )
    
    def _calculate_transition_diversity(self) -> float:
        """ì „ì´ ë‹¤ì–‘ì„± ê³„ì‚° (ëŒ€í™”ë³„ ìƒíƒœ ë³€í™”)"""
        # ëŒ€í™”ë³„ë¡œ ê·¸ë£¹í™”
        dialogues = defaultdict(list)
        for ann in self.annotations:
            dialogue_id = ann.get('dialogue_id')
            turn_id = ann.get('turn_id')
            if dialogue_id and turn_id is not None:
                dialogues[dialogue_id].append((turn_id, ann))
        
        transitions = []
        for dialogue_id, turns in dialogues.items():
            # turn_idë¡œ ì •ë ¬ (ë¬¸ìì—´/ì •ìˆ˜ í˜¼ì¬ ì²˜ë¦¬)
            try:
                turns.sort(key=lambda x: int(x[0]) if x[0] is not None else -1)
            except (ValueError, TypeError):
                turns.sort(key=lambda x: str(x[0]) if x[0] is not None else "")
            
            # ì—°ì†ëœ í„´ ê°„ ìƒíƒœ ë³€í™” ê³„ì‚°
            for i in range(len(turns) - 1):
                current_slots = set(turns[i][1]['normalized_slots'].keys())
                next_slots = set(turns[i+1][1]['normalized_slots'].keys())
                
                # ìŠ¬ë¡¯ ì§‘í•© ë³€í™” ì •ë„
                added_slots = next_slots - current_slots
                removed_slots = current_slots - next_slots
                
                transition_change = len(added_slots) + len(removed_slots)
                transitions.append(transition_change)
        
        return np.std(transitions) if transitions else 0.0
    
    def visualize_state_space(self, save_name: str = None, results_dir: Path = None):
        """ìƒíƒœ ê³µê°„ ì‹œê°í™”"""
        if self.state_vectors is None:
            self.extract_state_vectors()
        
        if len(self.state_vectors) < 2:
            print(f"ê²½ê³ : {self.annotation_type}ì˜ ìƒíƒœê°€ ë¶€ì¡±í•˜ì—¬ ì‹œê°í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
            return None
        
        # t-SNE ì°¨ì› ì¶•ì†Œ
        try:
            perplexity = min(30, len(self.state_vectors) // 3)
            tsne = TSNE(n_components=2, random_state=42, perplexity=max(5, perplexity))
            embeddings_2d = tsne.fit_transform(self.state_vectors)
        except:
            # t-SNE ì‹¤íŒ¨ ì‹œ PCA ì‚¬ìš©
            pca = PCA(n_components=2)
            embeddings_2d = pca.fit_transform(self.state_vectors)
        
        # ì‹œê°í™”
        plt.figure(figsize=(10, 8))
        scatter = plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], 
                           alpha=0.6, s=50, c=range(len(embeddings_2d)), cmap='viridis')
        plt.title(f'State Space Visualization - {self.annotation_type}\n'
                 f'States: {len(self.state_vectors)}, Unique: {len(np.unique(self.state_vectors, axis=0))}')
        plt.xlabel('Dimension 1')
        plt.ylabel('Dimension 2')
        plt.colorbar(scatter, label='Sample Index')
        
        if save_name and results_dir:
            save_path = results_dir / f"{save_name}_{self.annotation_type}_state_space.png"
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ì‹œê°í™” ì €ì¥: {save_path}")
        
        plt.tight_layout()
        plt.show()
        
        return embeddings_2d

class RealDataGeneralizationTester:
    """ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ ì¼ë°˜í™” ëŠ¥ë ¥ í…ŒìŠ¤í„°"""
    
    def __init__(self, analyzer: RealDataStateSpaceAnalyzer):
        self.analyzer = analyzer
        self.annotation_type = analyzer.annotation_type
        
    def test_cross_domain_generalization(self) -> Dict[str, float]:
        """ë„ë©”ì¸ ê°„ ì¼ë°˜í™” ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        print(f"[{self.annotation_type}] ë„ë©”ì¸ ê°„ ì¼ë°˜í™” í…ŒìŠ¤íŠ¸ ì¤‘...")
        
        # ë„ë©”ì¸ë³„ë¡œ annotation ë¶„ë¥˜
        domain_data = defaultdict(list)
        
        for ann in self.analyzer.annotations:
            # ìŠ¬ë¡¯ì—ì„œ ë„ë©”ì¸ ì¶”ì •
            domains = self._identify_domains(ann['normalized_slots'])
            for domain in domains:
                domain_data[domain].append(ann)
        
        if len(domain_data) < 2:
            print("ì¶©ë¶„í•œ ë„ë©”ì¸ì´ ì—†ì–´ ê¸°ë³¸ê°’ ë°˜í™˜")
            return {"single_domain": 0.5}
        
        results = {}
        
        # ê° ë„ë©”ì¸ì—ì„œ ë‹¤ë¥¸ ë„ë©”ì¸ìœ¼ë¡œ ì¼ë°˜í™” ëŠ¥ë ¥ ì¸¡ì •
        for source_domain, source_data in domain_data.items():
            if len(source_data) < 10:
                continue
                
            for target_domain, target_data in domain_data.items():
                if source_domain == target_domain or len(target_data) < 10:
                    continue
                
                # ì†ŒìŠ¤ ë„ë©”ì¸ì—ì„œ í•™ìŠµí•œ íŒ¨í„´ì´ íƒ€ê²Ÿ ë„ë©”ì¸ì— ì–¼ë§ˆë‚˜ ì ìš© ê°€ëŠ¥í•œì§€
                generalization_score = self._calculate_domain_overlap(source_data, target_data)
                results[f"{source_domain}_to_{target_domain}"] = generalization_score
        
        return results
    
    def _identify_domains(self, normalized_slots: Dict[str, str]) -> List[str]:
        """ìŠ¬ë¡¯ì—ì„œ ë„ë©”ì¸ ì‹ë³„"""
        domains = set()
        
        # ë„ë©”ì¸ íŠ¹í™” ìŠ¬ë¡¯ë“¤
        domain_indicators = {
            'restaurant': ['food', 'cuisine', 'restaurant', 'meal'],
            'hotel': ['hotel', 'accommodation', 'room', 'stay'],
            'train': ['train', 'railway', 'departure', 'arrival'],
            'taxi': ['taxi', 'transport', 'car'],
            'attraction': ['attraction', 'tourist', 'visit']
        }
        
        for slot_name in normalized_slots.keys():
            slot_lower = slot_name.lower()
            for domain, indicators in domain_indicators.items():
                if any(indicator in slot_lower for indicator in indicators):
                    domains.add(domain)
        
        # ê¸°ë³¸ ë„ë©”ì¸ í• ë‹¹
        if not domains:
            domains.add('general')
        
        return list(domains)
    
    def _calculate_domain_overlap(self, source_data: List[dict], target_data: List[dict]) -> float:
        """ë„ë©”ì¸ ê°„ íŒ¨í„´ ì¤‘ì²©ë„ ê³„ì‚°"""
        # ì†ŒìŠ¤ ë„ë©”ì¸ì˜ ìŠ¬ë¡¯ íŒ¨í„´
        source_patterns = set()
        for ann in source_data:
            pattern = tuple(sorted(ann['normalized_slots'].keys()))
            source_patterns.add(pattern)
        
        # íƒ€ê²Ÿ ë„ë©”ì¸ì˜ ìŠ¬ë¡¯ íŒ¨í„´
        target_patterns = set()
        for ann in target_data:
            pattern = tuple(sorted(ann['normalized_slots'].keys()))
            target_patterns.add(pattern)
        
        # ì¤‘ì²©ë„ ê³„ì‚°
        if not source_patterns or not target_patterns:
            return 0.0
        
        overlap = len(source_patterns & target_patterns)
        total = len(source_patterns | target_patterns)
        
        return overlap / total if total > 0 else 0.0
    
    def test_few_shot_capability(self, shots_list: List[int] = [1, 5, 10, 20]) -> Dict[int, float]:
        """Few-shot í•™ìŠµ ëŠ¥ë ¥ í…ŒìŠ¤íŠ¸"""
        print(f"[{self.annotation_type}] Few-shot ëŠ¥ë ¥ í…ŒìŠ¤íŠ¸ ì¤‘...")
        
        results = {}
        
        # ì „ì²´ íŒ¨í„´ ì¶”ì¶œ
        all_patterns = []
        for ann in self.analyzer.annotations:
            pattern = tuple(sorted(ann['normalized_slots'].keys()))
            all_patterns.append(pattern)
        
        pattern_counts = Counter(all_patterns)
        
        for n_shots in shots_list:
            if len(pattern_counts) < n_shots:
                results[n_shots] = 0.0
                continue
            
            # nê°œ ìƒ˜í”Œë¡œ ì–¼ë§ˆë‚˜ ë§ì€ íŒ¨í„´ì„ ì»¤ë²„í•  ìˆ˜ ìˆëŠ”ì§€
            total_patterns = len(pattern_counts)
            
            # ë¹ˆë„ ê¸°ë°˜ ìƒ˜í”Œë§ìœ¼ë¡œ ì»¤ë²„ë¦¬ì§€ ê³„ì‚°
            sorted_patterns = pattern_counts.most_common()
            covered_patterns = min(n_shots, len(sorted_patterns))
            
            # ê°€ì¤‘ ì»¤ë²„ë¦¬ì§€ (ë¹ˆë„ê°€ ë†’ì€ íŒ¨í„´ì¼ìˆ˜ë¡ ë†’ì€ ì ìˆ˜)
            covered_frequency = sum(count for _, count in sorted_patterns[:covered_patterns])
            total_frequency = sum(pattern_counts.values())
            
            coverage_score = covered_frequency / total_frequency if total_frequency > 0 else 0.0
            results[n_shots] = coverage_score
        
        return results
    
    def test_noise_robustness(self, noise_levels: List[float] = [0.1, 0.2, 0.3, 0.4, 0.5]) -> float:
        """ë…¸ì´ì¦ˆ ê²¬ê³ ì„± í…ŒìŠ¤íŠ¸"""
        print(f"[{self.annotation_type}] ë…¸ì´ì¦ˆ ê²¬ê³ ì„± í…ŒìŠ¤íŠ¸ ì¤‘...")
        
        if not self.analyzer.annotations:
            return 0.0
        
        # ì›ë³¸ íŒ¨í„´ ë‹¤ì–‘ì„±
        original_patterns = set()
        for ann in self.analyzer.annotations:
            pattern = tuple(sorted(ann['normalized_slots'].keys()))
            original_patterns.add(pattern)
        
        original_diversity = len(original_patterns)
        
        robustness_scores = []
        
        for noise_level in noise_levels:
            # ë…¸ì´ì¦ˆ ì¶”ê°€ëœ íŒ¨í„´ ìƒì„±
            noisy_patterns = set()
            
            for ann in self.analyzer.annotations:
                slots = list(ann['normalized_slots'].keys())
                
                # ë…¸ì´ì¦ˆ: ì¼ë¶€ ìŠ¬ë¡¯ ì œê±°
                if np.random.random() < noise_level and len(slots) > 1:
                    n_remove = max(1, int(len(slots) * noise_level))
                    remaining_slots = np.random.choice(slots, 
                                                     size=len(slots) - n_remove, 
                                                     replace=False)
                    pattern = tuple(sorted(remaining_slots))
                else:
                    pattern = tuple(sorted(slots))
                
                noisy_patterns.add(pattern)
            
            # ë…¸ì´ì¦ˆ í›„ ë‹¤ì–‘ì„± ìœ ì§€ ì •ë„
            noisy_diversity = len(noisy_patterns)
            robustness = noisy_diversity / original_diversity if original_diversity > 0 else 0.0
            robustness_scores.append(robustness)
        
        return np.mean(robustness_scores)
    
    def calculate_generalization_metrics(self) -> GeneralizationMetrics:
        """ì¼ë°˜í™” ë©”íŠ¸ë¦­ ì¢…í•© ê³„ì‚°"""
        print(f"[{self.annotation_type}] ì¼ë°˜í™” ë©”íŠ¸ë¦­ ê³„ì‚° ì¤‘...")
        
        cross_domain = self.test_cross_domain_generalization()
        few_shot = self.test_few_shot_capability()
        noise_robustness = self.test_noise_robustness()
        
        # Sample efficiency (íŒ¨í„´ ë‹¤ì–‘ì„± ëŒ€ë¹„ ë°ì´í„° í¬ê¸°)
        unique_patterns = set()
        for ann in self.analyzer.annotations:
            pattern = tuple(sorted(ann['normalized_slots'].keys()))
            unique_patterns.add(pattern)
        
        sample_efficiency = len(unique_patterns) / len(self.analyzer.annotations) if self.analyzer.annotations else 0.0
        
        # Unseen combination handling (ë³µì¡í•œ íŒ¨í„´ ì²˜ë¦¬ ëŠ¥ë ¥)
        pattern_complexities = [len(pattern) for pattern in unique_patterns]
        avg_complexity = np.mean(pattern_complexities) if pattern_complexities else 0.0
        unseen_combination_handling = min(1.0, avg_complexity / 5.0)  # ì •ê·œí™”
        
        return GeneralizationMetrics(
            cross_domain_performance=cross_domain,
            domain_adaptation_speed={"fast": 1 if len(cross_domain) > 2 else 0},
            few_shot_accuracy=few_shot,
            sample_efficiency=sample_efficiency,
            noise_robustness=noise_robustness,
            distribution_shift_robustness=np.mean(list(cross_domain.values())) if cross_domain else 0.0,
            unseen_combination_handling=unseen_combination_handling
        )

def run_real_richness_generalization_pipeline():
    """ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ ìƒíƒœ ê³µê°„ í’ë¶€í•¨ â†’ ì¼ë°˜í™” ëŠ¥ë ¥ ê²€ì¦ íŒŒì´í”„ë¼ì¸"""
    
    # ê²°ê³¼ ì €ì¥ ê²½ë¡œ ì„¤ì •
    results_dir = Path("mdp/rl_tests/state_richness_results")
    results_dir.mkdir(parents=True, exist_ok=True)
    
    print("="*80)
    print("ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ ìƒíƒœ ê³µê°„ í’ë¶€í•¨ â†’ ì¼ë°˜í™” ëŠ¥ë ¥ ê²€ì¦ íŒŒì´í”„ë¼ì¸")
    print("="*80)
    
    # ë°ì´í„° ë¡œë“œ (ê²½ë¡œ ìë™ íƒì§€)
    possible_paths = [
        Path("../processed_annotations.json"),           # rl_testsì—ì„œ ì‹¤í–‰ ì‹œ
        Path("mdp/processed_annotations.json"),          # í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ ì‹¤í–‰ ì‹œ
        Path("processed_annotations.json"),              # ê°™ì€ í´ë”ì—ì„œ ì‹¤í–‰ ì‹œ
        Path("../../mdp/processed_annotations.json")     # ë” ê¹Šì€ í´ë”ì—ì„œ ì‹¤í–‰ ì‹œ
    ]
    
    data_path = None
    for path in possible_paths:
        if path.exists():
            data_path = path
            break
    
    if data_path is None:
        print("ë‹¤ìŒ ìœ„ì¹˜ì—ì„œ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤:")
        for path in possible_paths:
            print(f"  - {path.absolute()}")
        raise FileNotFoundError("processed_annotations.json íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    print(f"ë°ì´í„° ë¡œë“œ ì¤‘: {data_path}")
    with open(data_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    canonical_map = data.get('canonical_map', {})
    human_annotations = data.get('human_annotations', [])
    llm_annotations = data.get('llm_annotations', [])
    
    print(f"ë¡œë“œëœ ë°ì´í„°:")
    print(f"  - Human annotations: {len(human_annotations)}ê°œ")
    print(f"  - LLM annotations: {len(llm_annotations)}ê°œ")
    print(f"  - Canonical map: {len(canonical_map)}ê°œ ë§¤í•‘")
    
    # 1ë‹¨ê³„: ìƒíƒœ ê³µê°„ í’ë¶€í•¨ ë¶„ì„
    print("\n1ë‹¨ê³„: ìƒíƒœ ê³µê°„ í’ë¶€í•¨ ë¶„ì„")
    print("-" * 50)
    
    human_analyzer = RealDataStateSpaceAnalyzer(human_annotations, "Human", canonical_map)
    llm_analyzer = RealDataStateSpaceAnalyzer(llm_annotations, "LLM", canonical_map)
    
    human_richness = human_analyzer.calculate_richness_metrics()
    llm_richness = llm_analyzer.calculate_richness_metrics()
    
    # ìƒíƒœ ê³µê°„ ì‹œê°í™”
    human_analyzer.visualize_state_space("richness_analysis", results_dir)
    llm_analyzer.visualize_state_space("richness_analysis", results_dir)
    
    # 2ë‹¨ê³„: ì¼ë°˜í™” ëŠ¥ë ¥ í…ŒìŠ¤íŠ¸
    print("\n2ë‹¨ê³„: ì¼ë°˜í™” ëŠ¥ë ¥ í…ŒìŠ¤íŠ¸")
    print("-" * 50)
    
    human_tester = RealDataGeneralizationTester(human_analyzer)
    llm_tester = RealDataGeneralizationTester(llm_analyzer)
    
    human_generalization = human_tester.calculate_generalization_metrics()
    llm_generalization = llm_tester.calculate_generalization_metrics()
    
    # 3ë‹¨ê³„: ìƒê´€ê´€ê³„ ë¶„ì„
    print("\n3ë‹¨ê³„: ìƒê´€ê´€ê³„ ë¶„ì„")
    print("-" * 50)
    
    correlation_analysis = analyze_real_richness_generalization_correlation(
        human_richness, llm_richness,
        human_generalization, llm_generalization
    )
    
    # 4ë‹¨ê³„: ê²°ê³¼ ì¢…í•© ë° ì‹œê°í™”
    print("\n4ë‹¨ê³„: ê²°ê³¼ ì¢…í•©")
    print("-" * 50)
    
    results = compile_real_results(
        human_richness, llm_richness,
        human_generalization, llm_generalization,
        correlation_analysis
    )
    
    # ì‹œê°í™”
    visualize_real_analysis_results(human_richness, llm_richness, 
                                   human_generalization, llm_generalization, results_dir)
    
    # ê²°ê³¼ ì €ì¥
    results_path = results_dir / 'state_richness_generalization_results.json'
    with open(results_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, default=str, ensure_ascii=False)
    
    print(f"\nê²°ê³¼ ì €ì¥: {results_path}")
    
    return results

def analyze_real_richness_generalization_correlation(human_richness: StateRichnessMetrics, 
                                                   llm_richness: StateRichnessMetrics,
                                                   human_gen: GeneralizationMetrics,
                                                   llm_gen: GeneralizationMetrics):
    """ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ í’ë¶€í•¨-ì¼ë°˜í™” ìƒê´€ê´€ê³„ ë¶„ì„"""
    
    print("í’ë¶€í•¨-ì¼ë°˜í™” ìƒê´€ê´€ê³„ ë¶„ì„ ì¤‘...")
    
    # í’ë¶€í•¨ ì ìˆ˜ ê³„ì‚°
    def calculate_richness_score(metrics: StateRichnessMetrics) -> float:
        return (
            (metrics.state_entropy / 10.0) * 0.20 +  # ì •ê·œí™”
            (metrics.effective_dimensionality / 50.0) * 0.20 +
            metrics.coverage_ratio * 0.15 +
            (metrics.density_uniformity + 1.0) / 2.0 * 0.15 +  # -1~1ì„ 0~1ë¡œ
            (metrics.manifold_complexity / 2.0) * 0.10 +
            (metrics.slot_vocabulary_size / 100.0) * 0.10 +
            (metrics.slot_value_combinations / 50.0) * 0.10
        )
    
    # ì¼ë°˜í™” ì ìˆ˜ ê³„ì‚°
    def calculate_generalization_score(metrics: GeneralizationMetrics) -> float:
        cross_domain_avg = np.mean(list(metrics.cross_domain_performance.values())) if metrics.cross_domain_performance else 0.0
        few_shot_avg = np.mean(list(metrics.few_shot_accuracy.values())) if metrics.few_shot_accuracy else 0.0
        
        return (
            cross_domain_avg * 0.30 +
            few_shot_avg * 0.25 +
            metrics.sample_efficiency * 0.20 +
            metrics.noise_robustness * 0.15 +
            metrics.unseen_combination_handling * 0.10
        )
    
    human_richness_score = calculate_richness_score(human_richness)
    llm_richness_score = calculate_richness_score(llm_richness)
    human_gen_score = calculate_generalization_score(human_gen)
    llm_gen_score = calculate_generalization_score(llm_gen)
    
    # ìƒê´€ê´€ê³„ ê³„ì‚°
    richness_scores = [human_richness_score, llm_richness_score]
    generalization_scores = [human_gen_score, llm_gen_score]
    
    if len(set(richness_scores)) > 1 and len(set(generalization_scores)) > 1:
        from scipy.stats import pearsonr
        correlation_coeff, p_value = pearsonr(richness_scores, generalization_scores)
    else:
        correlation_coeff = 0.0
        p_value = 1.0
    
    return {
        'human_richness_score': human_richness_score,
        'llm_richness_score': llm_richness_score,
        'human_generalization_score': human_gen_score,
        'llm_generalization_score': llm_gen_score,
        'richness_advantage': llm_richness_score - human_richness_score,
        'generalization_advantage': llm_gen_score - human_gen_score,
        'correlation_coefficient': correlation_coeff,
        'p_value': p_value
    }

def visualize_real_analysis_results(human_richness: StateRichnessMetrics,
                                  llm_richness: StateRichnessMetrics,
                                  human_gen: GeneralizationMetrics,
                                  llm_gen: GeneralizationMetrics,
                                  results_dir: Path):
    """ì‹¤ì œ ë¶„ì„ ê²°ê³¼ ì‹œê°í™”"""
    
    fig = plt.figure(figsize=(18, 12))
    
    # 1. ì–´íœ˜ í¬ê¸° ë¹„êµ
    ax1 = plt.subplot(2, 3, 1)
    categories = ['Slot\nVocabulary', 'Value\nVocabulary', 'Slot-Value\nCombinations']
    human_vocab = [human_richness.slot_vocabulary_size, 
                   human_richness.value_vocabulary_size,
                   human_richness.slot_value_combinations]
    llm_vocab = [llm_richness.slot_vocabulary_size,
                 llm_richness.value_vocabulary_size,
                 llm_richness.slot_value_combinations]
    
    x = np.arange(len(categories))
    width = 0.35
    
    ax1.bar(x - width/2, human_vocab, width, label='Human', color='skyblue', alpha=0.8)
    ax1.bar(x + width/2, llm_vocab, width, label='LLM', color='orange', alpha=0.8)
    ax1.set_title('Vocabulary Richness')
    ax1.set_xticks(x)
    ax1.set_xticklabels(categories)
    ax1.set_ylabel('Count')
    ax1.legend()
    ax1.grid(True, alpha=0.3)
    
    # 2. ìƒíƒœ ê³µê°„ ë©”íŠ¸ë¦­
    ax2 = plt.subplot(2, 3, 2)
    richness_metrics = ['Unique\nStates', 'Coverage\nRatio', 'Effective\nDimension']
    human_richness_vals = [human_richness.unique_states / 100.0,  # ì •ê·œí™”
                          human_richness.coverage_ratio,
                          human_richness.effective_dimensionality / 50.0]
    llm_richness_vals = [llm_richness.unique_states / 100.0,
                        llm_richness.coverage_ratio,
                        llm_richness.effective_dimensionality / 50.0]
    
    x = np.arange(len(richness_metrics))
    ax2.bar(x - width/2, human_richness_vals, width, label='Human', color='skyblue', alpha=0.8)
    ax2.bar(x + width/2, llm_richness_vals, width, label='LLM', color='orange', alpha=0.8)
    ax2.set_title('State Space Richness')
    ax2.set_xticks(x)
    ax2.set_xticklabels(richness_metrics)
    ax2.set_ylabel('Normalized Score')
    ax2.legend()
    ax2.grid(True, alpha=0.3)
    
    # 3. Few-shot ì„±ëŠ¥
    ax3 = plt.subplot(2, 3, 3)
    human_few_shot = list(human_gen.few_shot_accuracy.values())
    llm_few_shot = list(llm_gen.few_shot_accuracy.values())
    shots = list(human_gen.few_shot_accuracy.keys())
    
    if human_few_shot and llm_few_shot:
        ax3.plot(shots, human_few_shot, 'o-', label='Human', color='skyblue', linewidth=2, markersize=8)
        ax3.plot(shots, llm_few_shot, 's-', label='LLM', color='orange', linewidth=2, markersize=8)
    
    ax3.set_title('Few-shot Learning Performance')
    ax3.set_xlabel('Number of Shots')
    ax3.set_ylabel('Coverage Score')
    ax3.legend()
    ax3.grid(True, alpha=0.3)
    
    # 4. ì¼ë°˜í™” ëŠ¥ë ¥ ì¢…í•©
    ax4 = plt.subplot(2, 3, 4)
    gen_metrics = ['Sample\nEfficiency', 'Noise\nRobustness', 'Unseen\nHandling']
    human_gen_vals = [human_gen.sample_efficiency,
                     human_gen.noise_robustness,
                     human_gen.unseen_combination_handling]
    llm_gen_vals = [llm_gen.sample_efficiency,
                   llm_gen.noise_robustness,
                   llm_gen.unseen_combination_handling]
    
    x = np.arange(len(gen_metrics))
    ax4.bar(x - width/2, human_gen_vals, width, label='Human', color='skyblue', alpha=0.8)
    ax4.bar(x + width/2, llm_gen_vals, width, label='LLM', color='orange', alpha=0.8)
    ax4.set_title('Generalization Capabilities')
    ax4.set_xticks(x)
    ax4.set_xticklabels(gen_metrics)
    ax4.set_ylabel('Score')
    ax4.legend()
    ax4.grid(True, alpha=0.3)
    
    # 5. ë„ë©”ì¸ ê°„ ì¼ë°˜í™”
    ax5 = plt.subplot(2, 3, 5)
    human_cross_domain = list(human_gen.cross_domain_performance.values())
    llm_cross_domain = list(llm_gen.cross_domain_performance.values())
    
    if human_cross_domain or llm_cross_domain:
        ax5.boxplot([human_cross_domain, llm_cross_domain], 
                   labels=['Human', 'LLM'],
                   patch_artist=True,
                   boxprops=dict(facecolor='lightblue', alpha=0.7))
    else:
        ax5.text(0.5, 0.5, 'No cross-domain data', ha='center', va='center', transform=ax5.transAxes)
    
    ax5.set_title('Cross-domain Performance')
    ax5.set_ylabel('Overlap Score')
    ax5.grid(True, alpha=0.3)
    
    # 6. ì¢…í•© ì ìˆ˜
    ax6 = plt.subplot(2, 3, 6)
    
    # ì§ì ‘ ê³„ì‚° (import ì˜¤ë¥˜ ìˆ˜ì •)
    human_total = (human_richness.state_entropy / 10 + human_gen.sample_efficiency) / 2
    llm_total = (llm_richness.state_entropy / 10 + llm_gen.sample_efficiency) / 2
    
    categories = ['Overall\nRichness', 'Overall\nGeneralization']
    human_scores = [human_total, human_gen.sample_efficiency]
    llm_scores = [llm_total, llm_gen.sample_efficiency]
    
    x = np.arange(len(categories))
    ax6.bar(x - width/2, human_scores, width, label='Human', color='skyblue', alpha=0.8)
    ax6.bar(x + width/2, llm_scores, width, label='LLM', color='orange', alpha=0.8)
    ax6.set_title('Overall Performance')
    ax6.set_xticks(x)
    ax6.set_xticklabels(categories)
    ax6.set_ylabel('Composite Score')
    ax6.legend()
    ax6.grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    # ì €ì¥
    save_path = results_dir / 'richness_generalization_analysis.png'
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"ì‹œê°í™” ì €ì¥: {save_path}")
    
    plt.show()

def compile_real_results(human_richness: StateRichnessMetrics,
                        llm_richness: StateRichnessMetrics,
                        human_gen: GeneralizationMetrics,
                        llm_gen: GeneralizationMetrics,
                        correlation_analysis: Dict):
    """ì‹¤ì œ ê²°ê³¼ ì»´íŒŒì¼"""
    
    # ê°€ì„¤ ê²€ì¦
    hypothesis_verified = (
        correlation_analysis['richness_advantage'] > 0 and 
        correlation_analysis['generalization_advantage'] > 0
    )
    
    # ì£¼ìš” ê°œì„  ì‚¬í•­
    key_improvements = {
        'vocabulary_size': {
            'slot_vocab': llm_richness.slot_vocabulary_size - human_richness.slot_vocabulary_size,
            'value_vocab': llm_richness.value_vocabulary_size - human_richness.value_vocabulary_size,
            'combinations': llm_richness.slot_value_combinations - human_richness.slot_value_combinations
        },
        'state_space': {
            'unique_states': llm_richness.unique_states - human_richness.unique_states,
            'coverage_ratio': llm_richness.coverage_ratio - human_richness.coverage_ratio,
            'entropy': llm_richness.state_entropy - human_richness.state_entropy
        },
        'generalization': {
            'sample_efficiency': llm_gen.sample_efficiency - human_gen.sample_efficiency,
            'noise_robustness': llm_gen.noise_robustness - human_gen.noise_robustness
        }
    }
    
    results = {
        'experiment_name': 'LLM annotation â†’ ë” í’ë¶€í•œ ìƒíƒœ ê³µê°„ â†’ ë” ë‚˜ì€ ì¼ë°˜í™”',
        'hypothesis_verified': hypothesis_verified,
        'data_summary': {
            'human_annotations_count': len(human_richness.__dict__),  # ê·¼ì‚¬ì¹˜
            'llm_annotations_count': len(llm_richness.__dict__),
            'canonical_map_used': True
        },
        'richness_metrics': {
            'human': human_richness.__dict__,
            'llm': llm_richness.__dict__
        },
        'generalization_metrics': {
            'human': {
                'cross_domain_performance': human_gen.cross_domain_performance,
                'few_shot_accuracy': human_gen.few_shot_accuracy,
                'sample_efficiency': human_gen.sample_efficiency,
                'noise_robustness': human_gen.noise_robustness,
                'unseen_combination_handling': human_gen.unseen_combination_handling
            },
            'llm': {
                'cross_domain_performance': llm_gen.cross_domain_performance,
                'few_shot_accuracy': llm_gen.few_shot_accuracy,
                'sample_efficiency': llm_gen.sample_efficiency,
                'noise_robustness': llm_gen.noise_robustness,
                'unseen_combination_handling': llm_gen.unseen_combination_handling
            }
        },
        'correlation_analysis': correlation_analysis,
        'key_improvements': key_improvements,
        'conclusion': (
            "âœ… LLM annotationì´ ë” í’ë¶€í•œ ìƒíƒœ ê³µê°„ì„ ì œê³µí•˜ì—¬ ë” ë‚˜ì€ ì¼ë°˜í™”ë¥¼ ë‹¬ì„±í•¨"
            if hypothesis_verified else
            "âŒ ê°€ì„¤ì´ ì¶©ë¶„íˆ ì§€ì§€ë˜ì§€ ì•ŠìŒ"
        )
    }
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"\n{'='*60}")
    print("ì‹¤í—˜ ê²°ê³¼ ìš”ì•½")
    print(f"{'='*60}")
    print(f"ê°€ì„¤ ê²€ì¦: {'âœ… ì„±ê³µ' if hypothesis_verified else 'âŒ ì‹¤íŒ¨'}")
    print(f"í’ë¶€í•¨ ìš°ìœ„: {correlation_analysis['richness_advantage']:.3f}")
    print(f"ì¼ë°˜í™” ìš°ìœ„: {correlation_analysis['generalization_advantage']:.3f}")
    
    print(f"\nì£¼ìš” ê°œì„  ì‚¬í•­:")
    print(f"  ìŠ¬ë¡¯ ì–´íœ˜ í¬ê¸°: +{key_improvements['vocabulary_size']['slot_vocab']}")
    print(f"  ê°’ ì–´íœ˜ í¬ê¸°: +{key_improvements['vocabulary_size']['value_vocab']}")
    print(f"  ê³ ìœ  ìƒíƒœ ìˆ˜: +{key_improvements['state_space']['unique_states']}")
    print(f"  ìƒ˜í”Œ íš¨ìœ¨ì„±: +{key_improvements['generalization']['sample_efficiency']:.3f}")
    print(f"  ë…¸ì´ì¦ˆ ê²¬ê³ ì„±: +{key_improvements['generalization']['noise_robustness']:.3f}")
    
    return results

if __name__ == "__main__":
    try:
        results = run_real_richness_generalization_pipeline()
        print("\nğŸ‰ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ!")
        print(f"ê²°ê³¼ íŒŒì¼ ìœ„ì¹˜: mdp/rl_tests/state_richness_results/")
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()