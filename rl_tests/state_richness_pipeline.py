#!/usr/bin/env python3
"""
State-Richness pipeline rev 4 (state classification ë° ëŒ€í‘œ state ì¶”ì¶œ í¬í•¨)
"""
import argparse, json, random, os
from pathlib import Path
from collections import Counter
from typing import Dict, List, Tuple
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.metrics import silhouette_score
from sklearn.cluster import KMeans
from scipy.stats import entropy
from scipy.spatial.distance import cdist

# --------------------- util --------------------------------------------------
def _to_py(x):
    if isinstance(x, (np.integer,)):
        return int(x)
    if isinstance(x, (np.floating,)):
        return float(x)
    return x

def vecs(recs: List[dict], slot2i: Dict[str,int], val2i: Dict[str,int]):
    dim=len(slot2i)+len(val2i); arr=[]
    for r in recs:
        v=np.zeros(dim,dtype=np.int8)
        for k in r["state_after"]: v[slot2i[k]]=1
        for vstr in map(str, r["state_after"].values()):
            v[len(slot2i)+val2i[vstr]]=1
        arr.append(v)
    return np.vstack(arr)

def find_optimal_clusters(X, max_clusters=None):
    """ì—˜ë³´ìš° ë°©ë²•, ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´, ì¹¼ë¦°ìŠ¤í‚¤-í•˜ë¼ë°”ì¦ˆ ì§€ìˆ˜ë¥¼ ì¢…í•©í•œ ìµœì  í´ëŸ¬ìŠ¤í„° ìˆ˜ ê²°ì •"""
    if len(X) < 3:
        return 1
    
    # ê³ ìœ í•œ ìƒíƒœ ìˆ˜ ê¸°ë°˜ìœ¼ë¡œ ë” í˜„ì‹¤ì ì¸ ìµœëŒ€ í´ëŸ¬ìŠ¤í„° ìˆ˜ ì„¤ì •
    unique_states = len(np.unique(X, axis=0))
    max_k = max_clusters or min(max(unique_states // 3, 8), unique_states - 1)
    max_k = max(2, min(max_k, len(X) - 1))
    
    if max_k < 2:
        return 1
    
    K_range = range(2, max_k + 1)
    silhouette_scores = []
    inertias = []
    calinski_scores = []
    
    for k in K_range:
        kmeans = KMeans(n_clusters=k, n_init=10, random_state=0)
        labels = kmeans.fit_predict(X)
        
        # ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´
        sil_score = silhouette_score(X, labels)
        silhouette_scores.append(sil_score)
        
        # ì—˜ë³´ìš° ë°©ë²•ì„ ìœ„í•œ inertia
        inertias.append(kmeans.inertia_)
        
        # ì¹¼ë¦°ìŠ¤í‚¤-í•˜ë¼ë°”ì¦ˆ ì§€ìˆ˜
        from sklearn.metrics import calinski_harabasz_score
        ch_score = calinski_harabasz_score(X, labels)
        calinski_scores.append(ch_score)
    
    if not silhouette_scores:
        return 2
    
    # ì—˜ë³´ìš° ë°©ë²•ìœ¼ë¡œ ê¸‰ê²©í•œ ë³€í™”ì  ì°¾ê¸°
    def find_elbow(inertias):
        if len(inertias) < 3:
            return 0
        
        # 2ì°¨ ì°¨ë¶„ìœ¼ë¡œ ë³€ê³¡ì  ì°¾ê¸°
        diffs = np.diff(inertias)
        second_diffs = np.diff(diffs)
        if len(second_diffs) == 0:
            return 0
        return np.argmax(second_diffs) + 2  # K_range ì‹œì‘ì´ 2ì´ë¯€ë¡œ
    
    # ê° ë°©ë²•ì˜ ì¶”ì²œê°’
    sil_optimal = K_range[np.argmax(silhouette_scores)]
    elbow_k = find_elbow(inertias)
    ch_optimal = K_range[np.argmax(calinski_scores)]
    
    # ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ìµœì¢… ê²°ì • (ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´ì— ë” í° ê°€ì¤‘ì¹˜)
    candidates = []
    if sil_optimal: candidates.extend([sil_optimal] * 3)  # ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´ 3ë°° ê°€ì¤‘
    if elbow_k and 2 <= elbow_k <= max_k: candidates.append(elbow_k)
    if ch_optimal: candidates.append(ch_optimal)
    
    if candidates:
        # ê°€ì¥ ë¹ˆë²ˆí•œ ê°’ ì„ íƒ, ë™ë¥ ì´ë©´ ì¤‘ê°„ê°’
        from collections import Counter
        counter = Counter(candidates)
        most_common = counter.most_common()
        
        if len(most_common) > 1 and most_common[0][1] == most_common[1][1]:
            # ë™ë¥ ì´ë©´ ë” í° í´ëŸ¬ìŠ¤í„° ìˆ˜ ì„ íƒ (ë” ì„¸ë¶„í™”)
            optimal_k = max([k for k, count in most_common if count == most_common[0][1]])
        else:
            optimal_k = most_common[0][0]
        
        # ìµœì†Œ 3ê°œ í´ëŸ¬ìŠ¤í„° ë³´ì¥ (ë‹¨ìˆœí•œ ì´ì§„ ë¶„ë¥˜ ë°©ì§€)
        optimal_k = max(3, min(optimal_k, max_k))
        return optimal_k
    
    return max(3, min(max_k, len(K_range) // 2 + 2))

def classify_states(X, recs, optimal_k):
    """ë‹¤ì¤‘ ë°©ë²•ìœ¼ë¡œ ìƒíƒœë“¤ì„ í´ëŸ¬ìŠ¤í„°ë§í•˜ê³  ê° í´ëŸ¬ìŠ¤í„°ì˜ ëŒ€í‘œ ìƒíƒœ ì„ íƒ"""
    if len(X) < 2:
        return {
            'labels': np.zeros(len(X)),
            'representatives': [0] if len(X) > 0 else [],
            'cluster_info': {},
            'n_clusters': 1 if len(X) > 0 else 0,
            'method': 'single_state'
        }
    
    # ì—¬ëŸ¬ í´ëŸ¬ìŠ¤í„°ë§ ë°©ë²• ì‹œë„
    methods = {}
    
    # 1. K-Means
    kmeans = KMeans(n_clusters=optimal_k, n_init=10, random_state=0)
    kmeans_labels = kmeans.fit_predict(X)
    kmeans_score = silhouette_score(X, kmeans_labels) if len(np.unique(kmeans_labels)) > 1 else -1
    methods['kmeans'] = (kmeans_labels, kmeans_score, kmeans.cluster_centers_)
    
    # 2. ê³„ì¸µì  í´ëŸ¬ìŠ¤í„°ë§
    try:
        from sklearn.cluster import AgglomerativeClustering
        agg = AgglomerativeClustering(n_clusters=optimal_k)
        agg_labels = agg.fit_predict(X)
        agg_score = silhouette_score(X, agg_labels) if len(np.unique(agg_labels)) > 1 else -1
        methods['hierarchical'] = (agg_labels, agg_score, None)
    except:
        pass
    
    # 3. DBSCAN (ë°€ë„ ê¸°ë°˜)
    try:
        from sklearn.cluster import DBSCAN
        # eps ìë™ ì¡°ì •
        from sklearn.neighbors import NearestNeighbors
        nn = NearestNeighbors(n_neighbors=4)
        nn.fit(X)
        distances, _ = nn.kneighbors(X)
        eps = np.percentile(distances[:, -1], 90)  # 90í¼ì„¼íƒ€ì¼ì„ epsë¡œ ì‚¬ìš©
        
        dbscan = DBSCAN(eps=eps, min_samples=max(3, len(X) // 50))
        dbscan_labels = dbscan.fit_predict(X)
        
        # ë…¸ì´ì¦ˆ í¬ì¸íŠ¸(-1)ê°€ ë„ˆë¬´ ë§ìœ¼ë©´ ì œì™¸
        if len(np.unique(dbscan_labels[dbscan_labels != -1])) >= 2:
            dbscan_score = silhouette_score(X[dbscan_labels != -1], dbscan_labels[dbscan_labels != -1])
            methods['dbscan'] = (dbscan_labels, dbscan_score, None)
    except:
        pass
    
    # ìµœê³  ì ìˆ˜ì˜ ë°©ë²• ì„ íƒ
    best_method = 'kmeans'
    best_score = methods['kmeans'][1]
    
    for method_name, (labels, score, centers) in methods.items():
        if score > best_score:
            best_method = method_name
            best_score = score
    
    labels, _, centroids = methods[best_method]
    
    # DBSCANì˜ ê²½ìš° ë…¸ì´ì¦ˆ í¬ì¸íŠ¸ ì²˜ë¦¬
    if best_method == 'dbscan':
        # ë…¸ì´ì¦ˆ í¬ì¸íŠ¸ë“¤ì„ ê°€ì¥ ê°€ê¹Œìš´ í´ëŸ¬ìŠ¤í„°ì— í• ë‹¹
        noise_mask = labels == -1
        if np.any(noise_mask):
            valid_labels = labels[~noise_mask]
            valid_points = X[~noise_mask]
            noise_points = X[noise_mask]
            
            # ê° ë…¸ì´ì¦ˆ í¬ì¸íŠ¸ë¥¼ ê°€ì¥ ê°€ê¹Œìš´ í´ëŸ¬ìŠ¤í„°ì— í• ë‹¹
            for i, noise_point in enumerate(noise_points):
                distances_to_clusters = []
                for cluster_id in np.unique(valid_labels):
                    cluster_points = valid_points[valid_labels == cluster_id]
                    min_dist = np.min(cdist([noise_point], cluster_points))
                    distances_to_clusters.append(min_dist)
                
                closest_cluster = np.unique(valid_labels)[np.argmin(distances_to_clusters)]
                labels[noise_mask][i] = closest_cluster
    
    # í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ ê³„ì‚° (centroidsê°€ ì—†ëŠ” ê²½ìš°)
    if centroids is None:
        unique_labels = np.unique(labels)
        centroids = []
        for cluster_id in unique_labels:
            cluster_points = X[labels == cluster_id]
            centroid = np.mean(cluster_points, axis=0)
            centroids.append(centroid)
        centroids = np.array(centroids)
    
    # ê° í´ëŸ¬ìŠ¤í„°ì˜ ëŒ€í‘œ ìƒíƒœ ì„ íƒ
    representatives = []
    cluster_info = {}
    actual_clusters = len(np.unique(labels))
    
    for i, cluster_id in enumerate(np.unique(labels)):
        cluster_mask = labels == cluster_id
        cluster_points = X[cluster_mask]
        cluster_indices = np.where(cluster_mask)[0]
        
        if len(cluster_points) == 0:
            continue
            
        # centroidì™€ì˜ ê±°ë¦¬ ê³„ì‚°
        if len(centroids) > i:
            distances = cdist([centroids[i]], cluster_points, metric='euclidean')[0]
        else:
            # centroidsê°€ ë¶€ì¡±í•œ ê²½ìš° í´ëŸ¬ìŠ¤í„° ì¤‘ì‹¬ ì§ì ‘ ê³„ì‚°
            cluster_center = np.mean(cluster_points, axis=0)
            distances = cdist([cluster_center], cluster_points, metric='euclidean')[0]
        
        rep_idx_in_cluster = np.argmin(distances)
        rep_idx_global = cluster_indices[rep_idx_in_cluster]
        
        representatives.append(rep_idx_global)
        
        # í´ëŸ¬ìŠ¤í„° ë‚´ ë‹¤ì–‘ì„± ê³„ì‚°
        intra_cluster_distances = cdist(cluster_points, cluster_points)
        diversity = np.mean(intra_cluster_distances[np.triu_indices_from(intra_cluster_distances, k=1)])
        
        # í´ëŸ¬ìŠ¤í„° ì •ë³´ ì €ì¥
        cluster_info[f'cluster_{cluster_id}'] = {
            'size': int(np.sum(cluster_mask)),
            'representative_idx': int(rep_idx_global),
            'representative_state': recs[rep_idx_global]['state_after'],
            'centroid_distance': float(distances[rep_idx_in_cluster]),
            'avg_distance_to_centroid': float(np.mean(distances)),
            'intra_cluster_diversity': float(diversity) if not np.isnan(diversity) else 0.0
        }
    
    return {
        'labels': labels,
        'representatives': representatives,
        'cluster_info': cluster_info,
        'n_clusters': actual_clusters,
        'centroids': centroids,
        'method': best_method,
        'silhouette_score': best_score
    }

def richness(X, slot2i, val2i):
    uniq=len(np.unique(X,axis=0))
    ent=entropy(list(Counter(map(bytes,X)).values()))
    cov=uniq/len(X)
    pca=PCA().fit(X)
    eff=(pca.explained_variance_ratio_.cumsum()>=0.95).argmax()+1
    dens=sep=0.0
    if uniq>10:
        km=KMeans(n_clusters=min(10,uniq//2),n_init=5,random_state=0).fit(X)
        dens=silhouette_score(X,km.labels_)
    if uniq>1:
        smp=np.unique(X,axis=0)[:1500]
        sep=float(np.linalg.norm(smp[:,None]-smp,axis=-1).mean())
    return dict(unique_states=uniq,state_entropy=ent,coverage_ratio=cov,
                effective_dim=eff,density_uniformity=dens,cluster_separation=sep,
                slot_vocab=len(slot2i),value_vocab=len(val2i))

def tsne_fig_with_clusters(X, labels, representatives, title, path):
    """í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ë¥¼ í¬í•¨í•œ t-SNE ì‹œê°í™”"""
    if len(X)<3: return
    try: 
        emb=TSNE(n_components=2,random_state=0,perplexity=min(30,len(X)//3)).fit_transform(X)
    except: 
        emb=PCA(n_components=2).fit_transform(X)
    
    plt.figure(figsize=(8,6))
    
    # í´ëŸ¬ìŠ¤í„°ë³„ë¡œ ë‹¤ë¥¸ ìƒ‰ìƒìœ¼ë¡œ í‘œì‹œ
    if labels is not None and len(np.unique(labels)) > 1:
        scatter = plt.scatter(emb[:,0], emb[:,1], c=labels, s=20, alpha=0.6, cmap='tab10')
        plt.colorbar(scatter, label='Cluster')
        
        # ëŒ€í‘œ ìƒíƒœë“¤ì„ ê°•ì¡° í‘œì‹œ
        if representatives:
            rep_emb = emb[representatives]
            plt.scatter(rep_emb[:,0], rep_emb[:,1], c='red', s=100, marker='*', 
                       edgecolors='black', linewidth=1, label='Representatives', alpha=0.9)
            
            # ëŒ€í‘œ ìƒíƒœì— ë²ˆí˜¸ í‘œì‹œ
            for i, (x, y) in enumerate(rep_emb):
                plt.annotate(f'R{i}', (x, y), xytext=(5, 5), textcoords='offset points',
                           fontsize=8, fontweight='bold', color='white')
        
        plt.legend()
    else:
        plt.scatter(emb[:,0], emb[:,1], s=20, alpha=0.6)
    
    plt.title(f'{title} State Space (Clustered)')
    plt.xlabel('t-SNE 1')
    plt.ylabel('t-SNE 2')
    plt.tight_layout()
    plt.savefig(path, dpi=300, bbox_inches='tight')
    plt.close()

def tsne_fig(X,title,path):
    """ê¸°ì¡´ t-SNE ì‹œê°í™” (í˜¸í™˜ì„± ìœ ì§€)"""
    tsne_fig_with_clusters(X, None, None, title, path)

# --------------------- main --------------------------------------------------
def main():
    ap=argparse.ArgumentParser()
    ap.add_argument("--annotations",required=True)
    ap.add_argument("--outdir",default="mdp/rl_tests/state_richness_results")
    ap.add_argument("--max_clusters", type=int, default=None, 
                    help="ìµœëŒ€ í´ëŸ¬ìŠ¤í„° ìˆ˜ (Noneì´ë©´ ìë™ ê²°ì •)")
    a=ap.parse_args(); Path(a.outdir).mkdir(parents=True,exist_ok=True)

    data=json.load(open(a.annotations))
    recs=data["annotations"] if isinstance(data,dict) else data
    H=[r for r in recs if r.get("annotation_type")=="human"]
    L=[r for r in recs if r.get("annotation_type")=="llm"]
    if not H or not L: raise RuntimeError("annotation_type ë¼ë²¨ì´ ë¶€ì¡±í•©ë‹ˆë‹¤.")

    slotH={s:i for i,s in enumerate(sorted({k for r in H for k in r["state_after"]}))}
    valH ={v:i for i,v in enumerate(sorted({str(x) for r in H for x in r["state_after"].values()}))}
    slotL={s:i for i,s in enumerate(sorted({k for r in L for k in r["state_after"]}))}
    valL ={v:i for i,v in enumerate(sorted({str(x) for r in L for x in r["state_after"].values()}))}

    Xh,Xl=vecs(H,slotH,valH),vecs(L,slotL,valL)
    Rh,Rl=richness(Xh,slotH,valH),richness(Xl,slotL,valL)

    # ìƒíƒœ ë¶„ë¥˜ ë° ëŒ€í‘œ ìƒíƒœ ì¶”ì¶œ
    print("ğŸ” Human states í´ëŸ¬ìŠ¤í„°ë§ ì¤‘...")
    optimal_k_h = find_optimal_clusters(Xh, a.max_clusters)
    classification_h = classify_states(Xh, H, optimal_k_h)
    print(f"   â”” ë°©ë²•: {classification_h['method']}, {classification_h['n_clusters']}ê°œ í´ëŸ¬ìŠ¤í„°, "
          f"ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´: {classification_h.get('silhouette_score', 0):.3f}")

    print("ğŸ” LLM states í´ëŸ¬ìŠ¤í„°ë§ ì¤‘...")
    optimal_k_l = find_optimal_clusters(Xl, a.max_clusters)
    classification_l = classify_states(Xl, L, optimal_k_l)
    print(f"   â”” ë°©ë²•: {classification_l['method']}, {classification_l['n_clusters']}ê°œ í´ëŸ¬ìŠ¤í„°, "
          f"ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´: {classification_l.get('silhouette_score', 0):.3f}")

    # í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ì‹œê°í™”
    tsne_fig_with_clusters(Xh, classification_h['labels'], classification_h['representatives'],
                          "Human", Path(a.outdir)/"state_space_human_clustered.png")
    tsne_fig_with_clusters(Xl, classification_l['labels'], classification_l['representatives'],
                          "LLM", Path(a.outdir)/"state_space_llm_clustered.png")

    # ê¸°ì¡´ ì‹œê°í™”ë„ ìœ ì§€
    tsne_fig(Xh,"Human",Path(a.outdir)/"state_space_human.png")
    tsne_fig(Xl,"LLM",  Path(a.outdir)/"state_space_llm.png")

    # bar plot (ê¸°ì¡´)
    feat=["unique_states","coverage_ratio","effective_dim"]
    plt.figure(figsize=(5,3))
    x=np.arange(len(feat));w=.35
    plt.bar(x-w/2,[Rh[f] for f in feat],w,label="Human")
    plt.bar(x+w/2,[Rl[f] for f in feat],w,label="LLM")
    plt.xticks(x,feat); plt.legend(); plt.tight_layout()
    plt.savefig(Path(a.outdir)/"richness_compare.png",dpi=300); plt.close()

    # í´ëŸ¬ìŠ¤í„° ìˆ˜ ë¹„êµ ì°¨íŠ¸
    plt.figure(figsize=(6,4))
    categories = ['Human', 'LLM']
    cluster_counts = [classification_h['n_clusters'], classification_l['n_clusters']]
    colors = ['#1f77b4', '#ff7f0e']
    
    bars = plt.bar(categories, cluster_counts, color=colors, alpha=0.7)
    plt.ylabel('Number of Clusters')
    plt.title('State Clusters Comparison')
    plt.ylim(0, max(cluster_counts) * 1.2)
    
    # ë§‰ëŒ€ ìœ„ì— ìˆ«ì í‘œì‹œ
    for bar, count in zip(bars, cluster_counts):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1, 
                str(count), ha='center', va='bottom', fontweight='bold')
    
    plt.tight_layout()
    plt.savefig(Path(a.outdir)/"cluster_comparison.png", dpi=300)
    plt.close()

    # ê²°ê³¼ ì €ì¥
    out=Path(a.outdir)/"state_richness_metrics.json"
    results = {
        "human": {
            "richness_metrics": {k:_to_py(v) for k,v in Rh.items()},
            "clustering": {
                "method": classification_h.get('method', 'unknown'),
                "silhouette_score": float(classification_h.get('silhouette_score', 0)),
                "n_clusters": int(classification_h['n_clusters']),
                "cluster_info": {k: {
                    **{kk: (vv if not isinstance(vv, dict) else vv) for kk, vv in v.items()},
                    "representative_state": v["representative_state"]
                } for k, v in classification_h['cluster_info'].items()},
                "representative_indices": [int(x) for x in classification_h['representatives']]
            }
        },
        "llm": {
            "richness_metrics": {k:_to_py(v) for k,v in Rl.items()},
            "clustering": {
                "method": classification_l.get('method', 'unknown'),
                "silhouette_score": float(classification_l.get('silhouette_score', 0)),
                "n_clusters": int(classification_l['n_clusters']),
                "cluster_info": {k: {
                    **{kk: (vv if not isinstance(vv, dict) else vv) for kk, vv in v.items()},
                    "representative_state": v["representative_state"]
                } for k, v in classification_l['cluster_info'].items()},
                "representative_indices": [int(x) for x in classification_l['representatives']]
            }
        }
    }
    
    out.write_text(json.dumps(results, indent=2, ensure_ascii=False))
    print("âœ” saved â†’",out)
    
    # ëŒ€í‘œ ìƒíƒœë“¤ ìš”ì•½ ì¶œë ¥
    print(f"\nğŸ“Š Human ëŒ€í‘œ ìƒíƒœë“¤ ({classification_h['method']} ë°©ë²•):")
    for i, (cluster_name, info) in enumerate(classification_h['cluster_info'].items()):
        diversity = info.get('intra_cluster_diversity', 0)
        print(f"  {cluster_name} (í¬ê¸°: {info['size']}, ë‹¤ì–‘ì„±: {diversity:.3f}): {info['representative_state']}")
    
    print(f"\nğŸ“Š LLM ëŒ€í‘œ ìƒíƒœë“¤ ({classification_l['method']} ë°©ë²•):")
    for i, (cluster_name, info) in enumerate(classification_l['cluster_info'].items()):
        diversity = info.get('intra_cluster_diversity', 0)
        print(f"  {cluster_name} (í¬ê¸°: {info['size']}, ë‹¤ì–‘ì„±: {diversity:.3f}): {info['representative_state']}")
    
    print(f"\nğŸ¯ í´ëŸ¬ìŠ¤í„°ë§ í’ˆì§ˆ:")
    print(f"  Human - ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´: {classification_h.get('silhouette_score', 0):.3f}")
    print(f"  LLM   - ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´: {classification_l.get('silhouette_score', 0):.3f}")

if __name__=="__main__":
    np.random.seed(0); random.seed(0); main()