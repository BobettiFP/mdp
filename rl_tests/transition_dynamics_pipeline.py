#!/usr/bin/env python3
"""
ì‹¤ì œ ë°ì´í„° ê¸°ë°˜ ì „ì´ ë™ì—­í•™ â†’ í•™ìŠµ ì•ˆì •ì„± ì¦ëª… íŒŒì´í”„ë¼ì¸
===========================================================
mdp/processed_annotations.json ë°ì´í„°ë¥¼ í™œìš©í•˜ì—¬
LLM annotationì´ ë” ì¼ê´€ëœ ì „ì´ ë™ì—­í•™ì„ ì œê³µí•˜ì—¬ 
ë” ì•ˆì •ì ì¸ í•™ìŠµì„ ë‹¬ì„±í•œë‹¤ëŠ” ê°€ì„¤ì„ ê²€ì¦
"""

import numpy as np
import torch
import torch.nn as nn
from sklearn.metrics import mutual_info_score
from sklearn.cluster import KMeans
from scipy.stats import entropy, pearsonr
from scipy.spatial.distance import pdist, squareform
import matplotlib.pyplot as plt
import seaborn as sns
import networkx as nx
from typing import Dict, List, Tuple, Set, Union, Any
from dataclasses import dataclass
from collections import defaultdict, Counter
import json
from pathlib import Path
import itertools

# ê²°ê³¼ ì €ì¥ ê²½ë¡œëŠ” ì‹¤í–‰ ì‹œì— ì„¤ì •

@dataclass
class TransitionDynamicsMetrics:
    """ì „ì´ ë™ì—­í•™ ë©”íŠ¸ë¦­"""
    # 1. ì¼ê´€ì„± ë©”íŠ¸ë¦­
    transition_consistency: float      # ê°™ì€ (s,a) â†’ ê°™ì€ s' ë¹ˆë„
    transition_predictability: float   # ì „ì´ ì˜ˆì¸¡ ì •í™•ë„
    transition_stability: float       # ì „ì´ í™•ë¥  ë¶„ì‚°
    
    # 2. êµ¬ì¡°ì  ë©”íŠ¸ë¦­
    transition_entropy: float          # ì „ì´ ë¶„í¬ ì—”íŠ¸ë¡œí”¼
    state_connectivity: float         # ìƒíƒœ ê°„ ì—°ê²°ì„±
    markov_property: float            # ë§ˆë¥´ì½”í”„ ì„±ì§ˆ ë§Œì¡±ë„
    
    # 3. ì •ë³´ ì´ë¡ ì  ë©”íŠ¸ë¦­
    mutual_information: float         # I(S_t; S_{t+1} | A_t)
    transition_complexity: float      # ì „ì´ ë³µì¡ë„
    causal_strength: float           # ì¸ê³¼ ê´€ê³„ ê°•ë„
    
    # 4. íŒ¨í„´ ë©”íŠ¸ë¦­
    transition_pattern_diversity: float # ì „ì´ íŒ¨í„´ ë‹¤ì–‘ì„±
    determinism_score: float         # ê²°ì •ë¡ ì  ì •ë„
    cyclic_patterns: float           # ìˆœí™˜ íŒ¨í„´ ì¡´ì¬

@dataclass
class LearningStabilityMetrics:
    """í•™ìŠµ ì•ˆì •ì„± ë©”íŠ¸ë¦­"""
    # 1. ìˆ˜ë ´ ì•ˆì •ì„±
    convergence_smoothness: float     # í•™ìŠµ ê³¡ì„  ë¶€ë“œëŸ¬ì›€
    convergence_predictability: float # ìˆ˜ë ´ ì˜ˆì¸¡ ê°€ëŠ¥ì„±
    final_variance: float            # ìµœì¢… ì„±ëŠ¥ ë¶„ì‚°
    
    # 2. í•™ìŠµ ê³¼ì • ì•ˆì •ì„±  
    learning_consistency: float      # í•™ìŠµ ì¼ê´€ì„±
    gradient_stability: float       # Gradient ì•ˆì •ì„± (ì‹œë®¬ë ˆì´ì…˜)
    exploration_stability: float    # íƒí—˜ ì•ˆì •ì„±
    
    # 3. ê²¬ê³ ì„±
    hyperparameter_sensitivity: float # í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¯¼ê°ë„
    initialization_robustness: float  # ì´ˆê¸°í™” ê²¬ê³ ì„±
    replay_consistency: float        # ë°˜ë³µ ì‹¤í—˜ ì¼ê´€ì„±

class TransitionDynamicsAnalyzer:
    """ì „ì´ ë™ì—­í•™ ë¶„ì„ê¸°"""
    
    def __init__(self, annotations: List[dict], annotation_type: str, canonical_map: Dict[str, str]):
        self.annotations = annotations
        self.annotation_type = annotation_type
        self.canonical_map = canonical_map
        self.transitions = []
        self.state_transitions = None
        self.transition_matrix = None
        
        print(f"[{self.annotation_type}] ì „ì´ ë™ì—­í•™ ë¶„ì„ê¸° ì´ˆê¸°í™”: {len(annotations)}ê°œ annotation")
        
    def _normalize_slot_name(self, slot_name: str) -> str:
        """ìŠ¬ë¡¯ ì´ë¦„ ì •ê·œí™” (canonical_map í™œìš©)"""
        # canonical_mapì— ì •ì˜ëœ ë§¤í•‘ ì‚¬ìš©
        if slot_name in self.canonical_map:
            return self.canonical_map[slot_name]
        
        # ê¸°ë³¸ ì •ê·œí™”
        normalized = slot_name.lower().replace("-", "_")
        
        # ë„ë©”ì¸ ì ‘ë‘ì‚¬ ì œê±°
        domain_prefixes = ["restaurant_", "hotel_", "train_", "taxi_", "attraction_", 
                          "bus_", "hospital_", "police_"]
        for prefix in domain_prefixes:
            if normalized.startswith(prefix):
                normalized = normalized[len(prefix):]
                break
        
        return normalized
    
    def _extract_slot_value(self, slot_data: Union[str, Dict]) -> str:
        """ìŠ¬ë¡¯ ê°’ ì¶”ì¶œ"""
        if isinstance(slot_data, str):
            return slot_data
        elif isinstance(slot_data, dict):
            return str(slot_data.get("value", str(slot_data)))
        elif isinstance(slot_data, list):
            return str(slot_data[0]) if slot_data else ""
        elif isinstance(slot_data, (int, float)):
            return str(slot_data)
        else:
            return str(slot_data)
    
    def extract_transitions(self) -> List[Dict]:
        """ìƒíƒœ-í–‰ë™-ë‹¤ìŒìƒíƒœ ì „ì´ ì¶”ì¶œ"""
        print(f"[{self.annotation_type}] ì „ì´ ë°ì´í„° ì¶”ì¶œ ì¤‘...")
        
        # ëŒ€í™”ë³„ë¡œ ì •ë¦¬
        dialogues = defaultdict(list)
        for ann in self.annotations:
            if ann.get('dialogue_id') and ann.get('slots'):
                # ì •ê·œí™”ëœ ìŠ¬ë¡¯ìœ¼ë¡œ ë³€í™˜
                normalized_slots = {}
                for slot_name, slot_value in ann['slots'].items():
                    normalized_slot = self._normalize_slot_name(slot_name)
                    extracted_value = self._extract_slot_value(slot_value)
                    normalized_slots[normalized_slot] = extracted_value
                
                if normalized_slots:
                    ann_copy = ann.copy()
                    ann_copy['normalized_slots'] = normalized_slots
                    dialogues[ann['dialogue_id']].append(ann_copy)
        
        transitions = []
        
        for dialogue_id, turns in dialogues.items():
            # turn_idë¡œ ì •ë ¬
            try:
                turns.sort(key=lambda x: int(x.get('turn_id', 0)) if x.get('turn_id') is not None else 0)
            except (ValueError, TypeError):
                turns.sort(key=lambda x: str(x.get('turn_id', '')) if x.get('turn_id') is not None else '')
            
            # ì—°ì†ëœ í„´ ê°„ ì „ì´ ì¶”ì¶œ
            for i in range(len(turns) - 1):
                current_turn = turns[i]
                next_turn = turns[i + 1]
                
                # í˜„ì¬ ìƒíƒœ (ìŠ¬ë¡¯ ì§‘í•©ì„ frozen setìœ¼ë¡œ)
                current_state = frozenset(current_turn['normalized_slots'].keys())
                next_state = frozenset(next_turn['normalized_slots'].keys())
                
                # í–‰ë™ (ìƒˆë¡œ ì¶”ê°€ëœ ìŠ¬ë¡¯ë“¤)
                action = next_state - current_state
                
                # ì „ì´ ê¸°ë¡
                transition = {
                    'current_state': current_state,
                    'action': action,
                    'next_state': next_state,
                    'dialogue_id': dialogue_id,
                    'turn_pair': (i, i+1),
                    'current_slots': current_turn['normalized_slots'],
                    'next_slots': next_turn['normalized_slots']
                }
                transitions.append(transition)
        
        self.transitions = transitions
        print(f"ì´ {len(transitions)}ê°œ ì „ì´ ì¶”ì¶œë¨")
        return transitions
    
    def build_transition_matrix(self) -> np.ndarray:
        """ì „ì´ í–‰ë ¬ êµ¬ì¶•"""
        print(f"[{self.annotation_type}] ì „ì´ í–‰ë ¬ êµ¬ì¶• ì¤‘...")
        
        if not self.transitions:
            self.extract_transitions()
        
        if not self.transitions:
            print("ì „ì´ ë°ì´í„°ê°€ ì—†ì–´ ë¹ˆ í–‰ë ¬ ë°˜í™˜")
            return np.array([[]])
        
        # ëª¨ë“  ê³ ìœ  ìƒíƒœ ìˆ˜ì§‘
        all_states = set()
        for transition in self.transitions:
            all_states.add(transition['current_state'])
            all_states.add(transition['next_state'])
        
        if len(all_states) < 2:
            print("ì¶©ë¶„í•œ ìƒíƒœê°€ ì—†ì–´ ê¸°ë³¸ í–‰ë ¬ ë°˜í™˜")
            return np.array([[1.0]])
        
        # ìƒíƒœë¥¼ ì¸ë±ìŠ¤ì— ë§¤í•‘
        state_to_idx = {state: i for i, state in enumerate(sorted(all_states, key=str))}
        n_states = len(all_states)
        
        # ì „ì´ ì¹´ìš´íŠ¸ í–‰ë ¬
        transition_counts = np.zeros((n_states, n_states))
        
        for transition in self.transitions:
            current_idx = state_to_idx[transition['current_state']]
            next_idx = state_to_idx[transition['next_state']]
            transition_counts[current_idx, next_idx] += 1
        
        # í™•ë¥  í–‰ë ¬ë¡œ ë³€í™˜
        transition_matrix = transition_counts.copy()
        for i in range(n_states):
            row_sum = transition_counts[i].sum()
            if row_sum > 0:
                transition_matrix[i] = transition_counts[i] / row_sum
            else:
                # ìê¸° ìì‹ ìœ¼ë¡œ ì „ì´ (í¡ìˆ˜ ìƒíƒœ)
                transition_matrix[i, i] = 1.0
        
        self.transition_matrix = transition_matrix
        self.state_to_idx = state_to_idx
        print(f"ì „ì´ í–‰ë ¬ í¬ê¸°: {n_states} x {n_states}")
        return transition_matrix
    
    def calculate_transition_consistency(self) -> float:
        """ì „ì´ ì¼ê´€ì„± ê³„ì‚°"""
        if not self.transitions:
            self.extract_transitions()
        
        if not self.transitions:
            return 0.0
        
        # ê°™ì€ (current_state, action) ìŒì— ëŒ€í•œ next_state ë¶„í¬ì˜ ì¼ê´€ì„±
        state_action_outcomes = defaultdict(list)
        
        for transition in self.transitions:
            key = (transition['current_state'], transition['action'])
            state_action_outcomes[key].append(transition['next_state'])
        
        consistency_scores = []
        for outcomes in state_action_outcomes.values():
            if len(outcomes) > 1:
                # ê°€ì¥ ë¹ˆë²ˆí•œ ê²°ê³¼ì˜ ë¹„ìœ¨
                outcome_counts = Counter(outcomes)
                most_common_count = outcome_counts.most_common(1)[0][1]
                consistency = most_common_count / len(outcomes)
                consistency_scores.append(consistency)
            else:
                consistency_scores.append(1.0)  # ë‹¨ì¼ ê´€ì°°ì€ ì™„ì „ ì¼ê´€
        
        return np.mean(consistency_scores) if consistency_scores else 0.0
    
    def calculate_transition_predictability(self) -> float:
        """ì „ì´ ì˜ˆì¸¡ ê°€ëŠ¥ì„± ê³„ì‚°"""
        if self.transition_matrix is None:
            self.build_transition_matrix()
        
        if self.transition_matrix.size == 0:
            return 0.0
        
        # ê° ìƒíƒœì—ì„œ ë‹¤ìŒ ìƒíƒœ ì˜ˆì¸¡ì˜ í™•ì‹¤ì„± (ì—”íŠ¸ë¡œí”¼ ê¸°ë°˜)
        predictability_scores = []
        
        for i in range(self.transition_matrix.shape[0]):
            row = self.transition_matrix[i]
            if row.sum() > 0:
                # 0ì´ ì•„ë‹Œ í™•ë¥ ë“¤ë§Œ ê³ ë ¤
                non_zero_probs = row[row > 0]
                if len(non_zero_probs) > 0:
                    row_entropy = entropy(non_zero_probs)
                    max_entropy = np.log(len(non_zero_probs))
                    if max_entropy > 0:
                        predictability = 1 - (row_entropy / max_entropy)
                    else:
                        predictability = 1.0
                    predictability_scores.append(predictability)
        
        return np.mean(predictability_scores) if predictability_scores else 0.0
    
    def calculate_markov_property(self) -> float:
        """ë§ˆë¥´ì½”í”„ ì„±ì§ˆ ë§Œì¡±ë„ ê³„ì‚°"""
        if not self.transitions:
            self.extract_transitions()
        
        if len(self.transitions) < 2:
            return 1.0
        
        # ëŒ€í™”ë³„ë¡œ ì—°ì†ëœ ì „ì´ ë¶„ì„
        dialogues = defaultdict(list)
        for transition in self.transitions:
            dialogues[transition['dialogue_id']].append(transition)
        
        markov_scores = []
        
        for dialogue_transitions in dialogues.values():
            if len(dialogue_transitions) >= 2:
                for i in range(len(dialogue_transitions) - 1):
                    t1 = dialogue_transitions[i]
                    t2 = dialogue_transitions[i + 1]
                    
                    # ì—°ì†ëœ ì „ì´ì—ì„œ ìƒíƒœ ì—°ê²°ì„± ì²´í¬
                    if t1['next_state'] == t2['current_state']:
                        markov_scores.append(1.0)
                    else:
                        markov_scores.append(0.0)
        
        return np.mean(markov_scores) if markov_scores else 1.0
    
    def calculate_mutual_information(self) -> float:
        """ìƒíƒœ ê°„ ìƒí˜¸ ì •ë³´ëŸ‰ ê³„ì‚°"""
        if not self.transitions:
            self.extract_transitions()
        
        if not self.transitions:
            return 0.0
        
        # í˜„ì¬ ìƒíƒœì™€ ë‹¤ìŒ ìƒíƒœ ê°„ ìƒí˜¸ ì •ë³´
        current_states = []
        next_states = []
        
        for transition in self.transitions:
            current_states.append(str(sorted(transition['current_state'])))
            next_states.append(str(sorted(transition['next_state'])))
        
        if len(set(current_states)) > 1 and len(set(next_states)) > 1:
            try:
                mi = mutual_info_score(current_states, next_states)
            except:
                mi = 0.0
        else:
            mi = 0.0
        
        return mi
    
    def calculate_determinism_score(self) -> float:
        """ê²°ì •ë¡ ì  ì •ë„ ê³„ì‚°"""
        if self.transition_matrix is None:
            self.build_transition_matrix()
        
        if self.transition_matrix.size == 0:
            return 0.0
        
        # ê° í–‰ì—ì„œ ìµœëŒ€ê°’ì˜ í‰ê·  (ë†’ì„ìˆ˜ë¡ ê²°ì •ë¡ ì )
        determinism_scores = []
        for i in range(self.transition_matrix.shape[0]):
            row = self.transition_matrix[i]
            if row.sum() > 0:
                max_prob = np.max(row)
                determinism_scores.append(max_prob)
        
        return np.mean(determinism_scores) if determinism_scores else 0.0
    
    def calculate_transition_dynamics_metrics(self) -> TransitionDynamicsMetrics:
        """ì „ì´ ë™ì—­í•™ ë©”íŠ¸ë¦­ ì¢…í•© ê³„ì‚°"""
        print(f"[{self.annotation_type}] ì „ì´ ë™ì—­í•™ ë©”íŠ¸ë¦­ ê³„ì‚° ì¤‘...")
        
        # ê¸°ë³¸ ë©”íŠ¸ë¦­ë“¤
        consistency = self.calculate_transition_consistency()
        predictability = self.calculate_transition_predictability()
        markov_prop = self.calculate_markov_property()
        mutual_info = self.calculate_mutual_information()
        determinism = self.calculate_determinism_score()
        
        # ì „ì´ ì•ˆì •ì„± (í™•ë¥  ë¶„ì‚°)
        if self.transition_matrix is not None and self.transition_matrix.size > 0:
            non_zero_probs = self.transition_matrix[self.transition_matrix > 0]
            stability = 1.0 / (1.0 + np.var(non_zero_probs)) if len(non_zero_probs) > 0 else 1.0
        else:
            stability = 0.0
        
        # ì „ì´ ì—”íŠ¸ë¡œí”¼
        if self.transition_matrix is not None and self.transition_matrix.size > 0:
            entropies = []
            for i in range(self.transition_matrix.shape[0]):
                row = self.transition_matrix[i]
                non_zero = row[row > 0]
                if len(non_zero) > 0:
                    entropies.append(entropy(non_zero))
            trans_entropy = np.mean(entropies) if entropies else 0.0
        else:
            trans_entropy = 0.0
        
        # ìƒíƒœ ì—°ê²°ì„±
        if self.transition_matrix is not None and self.transition_matrix.size > 1:
            # ê·¸ë˜í”„ë¡œ ë³€í™˜í•˜ì—¬ ì—°ê²°ì„± ì¸¡ì •
            try:
                G = nx.from_numpy_array(self.transition_matrix > 0, create_using=nx.DiGraph)
                if len(G.nodes) > 0:
                    connectivity = nx.average_clustering(G.to_undirected())
                else:
                    connectivity = 0.0
            except:
                connectivity = 0.0
        else:
            connectivity = 0.0
        
        # ì „ì´ ë³µì¡ì„±
        if self.transitions:
            unique_transitions = len(set(
                (transition['current_state'], transition['action'], transition['next_state'])
                for transition in self.transitions
            ))
            complexity = unique_transitions / len(self.transitions)
        else:
            complexity = 0.0
        
        # íŒ¨í„´ ë‹¤ì–‘ì„±
        if self.transitions:
            transition_patterns = set()
            for transition in self.transitions:
                pattern = (len(transition['current_state']), len(transition['action']), len(transition['next_state']))
                transition_patterns.add(pattern)
            pattern_diversity = len(transition_patterns) / len(self.transitions)
        else:
            pattern_diversity = 0.0
        
        # ìˆœí™˜ íŒ¨í„´ (ê°„ë‹¨í•œ ê·¼ì‚¬)
        cyclic_patterns = connectivity  # ì—°ê²°ì„±ì´ ë†’ìœ¼ë©´ ìˆœí™˜ ê°€ëŠ¥ì„±ë„ ë†’ìŒ
        
        return TransitionDynamicsMetrics(
            transition_consistency=consistency,
            transition_predictability=predictability,
            transition_stability=stability,
            transition_entropy=trans_entropy,
            state_connectivity=connectivity,
            markov_property=markov_prop,
            mutual_information=mutual_info,
            transition_complexity=complexity,
            causal_strength=mutual_info * consistency,
            transition_pattern_diversity=pattern_diversity,
            determinism_score=determinism,
            cyclic_patterns=cyclic_patterns
        )

class LearningStabilitySimulator:
    """í•™ìŠµ ì•ˆì •ì„± ì‹œë®¬ë ˆì´í„°"""
    
    def __init__(self, dynamics_analyzer: TransitionDynamicsAnalyzer):
        self.analyzer = dynamics_analyzer
        self.annotation_type = dynamics_analyzer.annotation_type
        
    def simulate_learning_curves(self, num_simulations: int = 10, num_episodes: int = 100) -> List[List[float]]:
        """í•™ìŠµ ê³¡ì„  ì‹œë®¬ë ˆì´ì…˜"""
        print(f"[{self.annotation_type}] í•™ìŠµ ê³¡ì„  ì‹œë®¬ë ˆì´ì…˜ ì¤‘...")
        
        curves = []
        
        # ì „ì´ ë™ì—­í•™ì˜ ì¼ê´€ì„±ì— ê¸°ë°˜í•œ ì‹œë®¬ë ˆì´ì…˜
        dynamics_metrics = self.analyzer.calculate_transition_dynamics_metrics()
        
        for sim in range(num_simulations):
            curve = []
            
            # ì´ˆê¸° ì„±ëŠ¥
            performance = 0.1
            
            for episode in range(num_episodes):
                # ì „ì´ ì¼ê´€ì„±ì´ ë†’ì„ìˆ˜ë¡ ì•ˆì •ì ì¸ í•™ìŠµ
                consistency_factor = dynamics_metrics.transition_consistency
                predictability_factor = dynamics_metrics.transition_predictability
                
                # í•™ìŠµë¥  (ì¼ê´€ì„±ê³¼ ì˜ˆì¸¡ê°€ëŠ¥ì„±ì— ë¹„ë¡€)
                learning_rate = 0.01 * (1 + consistency_factor + predictability_factor)
                
                # ë…¸ì´ì¦ˆ (ì¼ê´€ì„±ì´ ë†’ì„ìˆ˜ë¡ ë…¸ì´ì¦ˆ ê°ì†Œ)
                noise_std = 0.1 * (1 - consistency_factor)
                noise = np.random.normal(0, noise_std)
                
                # ì„±ëŠ¥ ì—…ë°ì´íŠ¸
                improvement = learning_rate * (1 - performance) + noise
                performance = max(0, min(1, performance + improvement))
                
                curve.append(performance)
            
            curves.append(curve)
        
        return curves
    
    def calculate_learning_stability_metrics(self) -> LearningStabilityMetrics:
        """í•™ìŠµ ì•ˆì •ì„± ë©”íŠ¸ë¦­ ê³„ì‚°"""
        print(f"[{self.annotation_type}] í•™ìŠµ ì•ˆì •ì„± ë©”íŠ¸ë¦­ ê³„ì‚° ì¤‘...")
        
        # í•™ìŠµ ê³¡ì„  ì‹œë®¬ë ˆì´ì…˜
        learning_curves = self.simulate_learning_curves()
        
        if not learning_curves:
            return LearningStabilityMetrics(
                convergence_smoothness=0.0, convergence_predictability=0.0, final_variance=1.0,
                learning_consistency=0.0, gradient_stability=0.0, exploration_stability=0.0,
                hyperparameter_sensitivity=1.0, initialization_robustness=0.0, replay_consistency=0.0
            )
        
        # ìˆ˜ë ´ ë¶€ë“œëŸ¬ì›€ (ë³€ë™ì„±ì˜ ì—­ìˆ˜)
        smoothness_scores = []
        for curve in learning_curves:
            if len(curve) > 1:
                differences = np.diff(curve)
                smoothness = 1.0 / (1.0 + np.var(differences))
                smoothness_scores.append(smoothness)
        
        convergence_smoothness = np.mean(smoothness_scores) if smoothness_scores else 0.0
        
        # ìˆ˜ë ´ ì˜ˆì¸¡ê°€ëŠ¥ì„± (ì‹œë®¬ë ˆì´ì…˜ ê°„ ì¼ê´€ì„±)
        final_performances = [curve[-1] for curve in learning_curves]
        convergence_predictability = 1.0 / (1.0 + np.var(final_performances))
        
        # ìµœì¢… ë¶„ì‚°
        final_variance = np.var(final_performances)
        
        # í•™ìŠµ ì¼ê´€ì„± (ì‹œë®¬ë ˆì´ì…˜ ê°„ ê³¡ì„  ìœ ì‚¬ì„±)
        if len(learning_curves) > 1:
            correlations = []
            for i in range(len(learning_curves)):
                for j in range(i+1, len(learning_curves)):
                    try:
                        corr, _ = pearsonr(learning_curves[i], learning_curves[j])
                        if not np.isnan(corr):
                            correlations.append(abs(corr))
                    except:
                        pass
            learning_consistency = np.mean(correlations) if correlations else 0.0
        else:
            learning_consistency = 1.0
        
        # Gradient ì•ˆì •ì„± (ì „ì´ ì¼ê´€ì„± ê¸°ë°˜ ê·¼ì‚¬)
        dynamics_metrics = self.analyzer.calculate_transition_dynamics_metrics()
        gradient_stability = dynamics_metrics.transition_consistency
        
        # íƒí—˜ ì•ˆì •ì„± (ì „ì´ ì˜ˆì¸¡ê°€ëŠ¥ì„± ê¸°ë°˜)
        exploration_stability = dynamics_metrics.transition_predictability
        
        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¯¼ê°ë„ (ì „ì´ ë³µì¡ì„±ì˜ ì—­ìˆ˜)
        hyperparameter_sensitivity = 1.0 - dynamics_metrics.transition_complexity
        
        # ì´ˆê¸°í™” ê²¬ê³ ì„± (ë§ˆë¥´ì½”í”„ ì„±ì§ˆ ê¸°ë°˜)
        initialization_robustness = dynamics_metrics.markov_property
        
        # ë°˜ë³µ ì‹¤í—˜ ì¼ê´€ì„±
        replay_consistency = learning_consistency
        
        return LearningStabilityMetrics(
            convergence_smoothness=convergence_smoothness,
            convergence_predictability=convergence_predictability,
            final_variance=final_variance,
            learning_consistency=learning_consistency,
            gradient_stability=gradient_stability,
            exploration_stability=exploration_stability,
            hyperparameter_sensitivity=hyperparameter_sensitivity,
            initialization_robustness=initialization_robustness,
            replay_consistency=replay_consistency
        )

def run_transition_dynamics_pipeline():
    """ì „ì´ ë™ì—­í•™ â†’ í•™ìŠµ ì•ˆì •ì„± ê²€ì¦ íŒŒì´í”„ë¼ì¸"""
    
    # ê²°ê³¼ ì €ì¥ ê²½ë¡œ ì„¤ì •
    results_dir = Path("mdp/rl_tests/transition_dynamics_results")
    results_dir.mkdir(parents=True, exist_ok=True)
    
    print("="*80)
    print("ì „ì´ ë™ì—­í•™ ì¼ê´€ì„± â†’ í•™ìŠµ ì•ˆì •ì„± ê²€ì¦ íŒŒì´í”„ë¼ì¸")
    print("="*80)
    
    # ë°ì´í„° ë¡œë“œ (ê²½ë¡œ ìë™ íƒì§€)
    possible_paths = [
        Path("../processed_annotations.json"),           # rl_testsì—ì„œ ì‹¤í–‰ ì‹œ
        Path("mdp/processed_annotations.json"),          # í”„ë¡œì íŠ¸ ë£¨íŠ¸ì—ì„œ ì‹¤í–‰ ì‹œ
        Path("processed_annotations.json"),              # ê°™ì€ í´ë”ì—ì„œ ì‹¤í–‰ ì‹œ
        Path("../../mdp/processed_annotations.json")     # ë” ê¹Šì€ í´ë”ì—ì„œ ì‹¤í–‰ ì‹œ
    ]
    
    data_path = None
    for path in possible_paths:
        if path.exists():
            data_path = path
            break
    
    if data_path is None:
        print("ë‹¤ìŒ ìœ„ì¹˜ì—ì„œ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤:")
        for path in possible_paths:
            print(f"  - {path.absolute()}")
        raise FileNotFoundError("processed_annotations.json íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    print(f"ë°ì´í„° ë¡œë“œ ì¤‘: {data_path}")
    with open(data_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    canonical_map = data.get('canonical_map', {})
    human_annotations = data.get('human_annotations', [])
    llm_annotations = data.get('llm_annotations', [])
    
    print(f"ë¡œë“œëœ ë°ì´í„°:")
    print(f"  - Human annotations: {len(human_annotations)}ê°œ")
    print(f"  - LLM annotations: {len(llm_annotations)}ê°œ")
    print(f"  - Canonical map: {len(canonical_map)}ê°œ ë§¤í•‘")
    
    # 1ë‹¨ê³„: ì „ì´ ë™ì—­í•™ ë¶„ì„
    print("\n1ë‹¨ê³„: ì „ì´ ë™ì—­í•™ ë¶„ì„")
    print("-" * 50)
    
    human_dynamics = TransitionDynamicsAnalyzer(human_annotations, "Human", canonical_map)
    llm_dynamics = TransitionDynamicsAnalyzer(llm_annotations, "LLM", canonical_map)
    
    human_dyn_metrics = human_dynamics.calculate_transition_dynamics_metrics()
    llm_dyn_metrics = llm_dynamics.calculate_transition_dynamics_metrics()
    
    # ì „ì´ í–‰ë ¬ ì‹œê°í™”
    visualize_transition_matrices(human_dynamics, llm_dynamics, results_dir)
    
    # 2ë‹¨ê³„: í•™ìŠµ ì•ˆì •ì„± ì‹œë®¬ë ˆì´ì…˜
    print("\n2ë‹¨ê³„: í•™ìŠµ ì•ˆì •ì„± ì‹œë®¬ë ˆì´ì…˜")
    print("-" * 50)
    
    human_simulator = LearningStabilitySimulator(human_dynamics)
    llm_simulator = LearningStabilitySimulator(llm_dynamics)
    
    human_stability_metrics = human_simulator.calculate_learning_stability_metrics()
    llm_stability_metrics = llm_simulator.calculate_learning_stability_metrics()
    
    # í•™ìŠµ ì•ˆì •ì„± ì‹œê°í™”
    visualize_learning_stability(human_simulator, llm_simulator, results_dir)
    
    # 3ë‹¨ê³„: ìƒê´€ê´€ê³„ ë¶„ì„
    print("\n3ë‹¨ê³„: ìƒê´€ê´€ê³„ ë¶„ì„")
    print("-" * 50)
    
    correlation_analysis = analyze_dynamics_stability_correlation(
        human_dyn_metrics, llm_dyn_metrics,
        human_stability_metrics, llm_stability_metrics
    )
    
    # 4ë‹¨ê³„: ê²°ê³¼ ì¢…í•©
    print("\n4ë‹¨ê³„: ê²°ê³¼ ì¢…í•©")
    print("-" * 50)
    
    results = compile_dynamics_stability_results(
        human_dyn_metrics, llm_dyn_metrics,
        human_stability_metrics, llm_stability_metrics,
        correlation_analysis
    )
    
    # ì¢…í•© ì‹œê°í™”
    visualize_comprehensive_results(human_dyn_metrics, llm_dyn_metrics,
                                  human_stability_metrics, llm_stability_metrics, results_dir)
    
    # ê²°ê³¼ ì €ì¥
    results_path = results_dir / 'transition_dynamics_stability_results.json'
    with open(results_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, default=str, ensure_ascii=False)
    
    print(f"\nê²°ê³¼ ì €ì¥: {results_path}")
    
    return results

def visualize_transition_matrices(human_dynamics: TransitionDynamicsAnalyzer,
                                llm_dynamics: TransitionDynamicsAnalyzer,
                                results_dir: Path):
    """ì „ì´ í–‰ë ¬ ì‹œê°í™”"""
    fig, axes = plt.subplots(1, 2, figsize=(15, 6))
    
    # Human transition matrix
    if human_dynamics.transition_matrix is not None and human_dynamics.transition_matrix.size > 1:
        im1 = axes[0].imshow(human_dynamics.transition_matrix, cmap='Blues', aspect='auto')
        axes[0].set_title(f'Human Annotation Transition Matrix\n({human_dynamics.transition_matrix.shape[0]} states)')
        axes[0].set_xlabel('Next State')
        axes[0].set_ylabel('Current State')
        plt.colorbar(im1, ax=axes[0])
    else:
        axes[0].text(0.5, 0.5, 'Insufficient\nTransition Data', ha='center', va='center', transform=axes[0].transAxes)
        axes[0].set_title('Human Annotation Transition Matrix')
    
    # LLM transition matrix
    if llm_dynamics.transition_matrix is not None and llm_dynamics.transition_matrix.size > 1:
        im2 = axes[1].imshow(llm_dynamics.transition_matrix, cmap='Oranges', aspect='auto')
        axes[1].set_title(f'LLM Annotation Transition Matrix\n({llm_dynamics.transition_matrix.shape[0]} states)')
        axes[1].set_xlabel('Next State')
        axes[1].set_ylabel('Current State')
        plt.colorbar(im2, ax=axes[1])
    else:
        axes[1].text(0.5, 0.5, 'Insufficient\nTransition Data', ha='center', va='center', transform=axes[1].transAxes)
        axes[1].set_title('LLM Annotation Transition Matrix')
    
    plt.tight_layout()
    
    save_path = results_dir / 'transition_matrices_comparison.png'
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"ì „ì´ í–‰ë ¬ ì‹œê°í™” ì €ì¥: {save_path}")
    
    plt.show()

def visualize_learning_stability(human_simulator: LearningStabilitySimulator,
                                llm_simulator: LearningStabilitySimulator,
                                results_dir: Path):
    """í•™ìŠµ ì•ˆì •ì„± ì‹œê°í™”"""
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    
    # í•™ìŠµ ê³¡ì„  ì‹œë®¬ë ˆì´ì…˜
    human_curves = human_simulator.simulate_learning_curves(num_simulations=10)
    llm_curves = llm_simulator.simulate_learning_curves(num_simulations=10)
    
    # 1. ê°œë³„ í•™ìŠµ ê³¡ì„ ë“¤
    for curve in human_curves:
        axes[0,0].plot(curve, color='blue', alpha=0.3, linewidth=1)
    for curve in llm_curves:
        axes[0,0].plot(curve, color='orange', alpha=0.3, linewidth=1)
    
    # í‰ê·  ê³¡ì„ 
    if human_curves:
        human_mean = np.mean(human_curves, axis=0)
        axes[0,0].plot(human_mean, color='blue', linewidth=3, label='Human (í‰ê· )')
    if llm_curves:
        llm_mean = np.mean(llm_curves, axis=0)
        axes[0,0].plot(llm_mean, color='orange', linewidth=3, label='LLM (í‰ê· )')
    
    axes[0,0].set_title('Learning Curve Simulations')
    axes[0,0].set_xlabel('Episode')
    axes[0,0].set_ylabel('Performance')
    axes[0,0].legend()
    axes[0,0].grid(True, alpha=0.3)
    
    # 2. ìµœì¢… ì„±ëŠ¥ ë¶„í¬
    if human_curves and llm_curves:
        human_final = [curve[-1] for curve in human_curves]
        llm_final = [curve[-1] for curve in llm_curves]
        
        axes[0,1].boxplot([human_final, llm_final], labels=['Human', 'LLM'])
        axes[0,1].set_title('Final Performance Distribution')
        axes[0,1].set_ylabel('Final Performance')
        axes[0,1].grid(True, alpha=0.3)
    
    # 3. í•™ìŠµ ë¶„ì‚° (ì—í”¼ì†Œë“œë³„)
    if human_curves and llm_curves:
        human_variance = np.var(human_curves, axis=0)
        llm_variance = np.var(llm_curves, axis=0)
        
        axes[1,0].plot(human_variance, color='blue', label='Human', linewidth=2)
        axes[1,0].plot(llm_variance, color='orange', label='LLM', linewidth=2)
        axes[1,0].set_title('Learning Variance Over Episodes')
        axes[1,0].set_xlabel('Episode')
        axes[1,0].set_ylabel('Variance')
        axes[1,0].legend()
        axes[1,0].grid(True, alpha=0.3)
    
    # 4. ìˆ˜ë ´ ì†ë„ ë¹„êµ
    if human_curves and llm_curves:
        human_convergence = []
        llm_convergence = []
        
        for curve in human_curves:
            # 90% ì„±ëŠ¥ ë‹¬ì„± ì‹œì 
            target = 0.9 * curve[-1]
            convergence_episode = len(curve)
            for i, perf in enumerate(curve):
                if perf >= target:
                    convergence_episode = i
                    break
            human_convergence.append(convergence_episode)
        
        for curve in llm_curves:
            target = 0.9 * curve[-1]
            convergence_episode = len(curve)
            for i, perf in enumerate(curve):
                if perf >= target:
                    convergence_episode = i
                    break
            llm_convergence.append(convergence_episode)
        
        axes[1,1].boxplot([human_convergence, llm_convergence], labels=['Human', 'LLM'])
        axes[1,1].set_title('Convergence Speed (Episodes to 90%)')
        axes[1,1].set_ylabel('Episodes')
        axes[1,1].grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    save_path = results_dir / 'learning_stability_analysis.png'
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"í•™ìŠµ ì•ˆì •ì„± ì‹œê°í™” ì €ì¥: {save_path}")
    
    plt.show()

def visualize_comprehensive_results(human_dyn: TransitionDynamicsMetrics,
                                  llm_dyn: TransitionDynamicsMetrics,
                                  human_stab: LearningStabilityMetrics,
                                  llm_stab: LearningStabilityMetrics,
                                  results_dir: Path):
    """ì¢…í•© ê²°ê³¼ ì‹œê°í™”"""
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    
    # 1. ì „ì´ ë™ì—­í•™ ë©”íŠ¸ë¦­
    dynamics_metrics = ['Consistency', 'Predictability', 'Stability', 'Markov\nProperty']
    human_dyn_vals = [human_dyn.transition_consistency, human_dyn.transition_predictability,
                     human_dyn.transition_stability, human_dyn.markov_property]
    llm_dyn_vals = [llm_dyn.transition_consistency, llm_dyn.transition_predictability,
                   llm_dyn.transition_stability, llm_dyn.markov_property]
    
    x = np.arange(len(dynamics_metrics))
    width = 0.35
    
    axes[0,0].bar(x - width/2, human_dyn_vals, width, label='Human', color='skyblue', alpha=0.8)
    axes[0,0].bar(x + width/2, llm_dyn_vals, width, label='LLM', color='orange', alpha=0.8)
    axes[0,0].set_title('Transition Dynamics Quality')
    axes[0,0].set_xticks(x)
    axes[0,0].set_xticklabels(dynamics_metrics)
    axes[0,0].set_ylabel('Score')
    axes[0,0].legend()
    axes[0,0].grid(True, alpha=0.3)
    
    # 2. í•™ìŠµ ì•ˆì •ì„± ë©”íŠ¸ë¦­
    stability_metrics = ['Convergence\nSmoothness', 'Learning\nConsistency', 'Gradient\nStability', 'Initialization\nRobustness']
    human_stab_vals = [human_stab.convergence_smoothness, human_stab.learning_consistency,
                      human_stab.gradient_stability, human_stab.initialization_robustness]
    llm_stab_vals = [llm_stab.convergence_smoothness, llm_stab.learning_consistency,
                    llm_stab.gradient_stability, llm_stab.initialization_robustness]
    
    x = np.arange(len(stability_metrics))
    axes[0,1].bar(x - width/2, human_stab_vals, width, label='Human', color='skyblue', alpha=0.8)
    axes[0,1].bar(x + width/2, llm_stab_vals, width, label='LLM', color='orange', alpha=0.8)
    axes[0,1].set_title('Learning Stability')
    axes[0,1].set_xticks(x)
    axes[0,1].set_xticklabels(stability_metrics)
    axes[0,1].set_ylabel('Score')
    axes[0,1].legend()
    axes[0,1].grid(True, alpha=0.3)
    
    # 3. ì •ë³´ ì´ë¡ ì  ë©”íŠ¸ë¦­
    info_metrics = ['Mutual\nInformation', 'Transition\nEntropy', 'Causal\nStrength']
    human_info_vals = [human_dyn.mutual_information, human_dyn.transition_entropy, human_dyn.causal_strength]
    llm_info_vals = [llm_dyn.mutual_information, llm_dyn.transition_entropy, llm_dyn.causal_strength]
    
    x = np.arange(len(info_metrics))
    axes[0,2].bar(x - width/2, human_info_vals, width, label='Human', color='skyblue', alpha=0.8)
    axes[0,2].bar(x + width/2, llm_info_vals, width, label='LLM', color='orange', alpha=0.8)
    axes[0,2].set_title('Information Theoretic Measures')
    axes[0,2].set_xticks(x)
    axes[0,2].set_xticklabels(info_metrics)
    axes[0,2].set_ylabel('Score')
    axes[0,2].legend()
    axes[0,2].grid(True, alpha=0.3)
    
    # 4. ë³µì¡ì„± vs ì•ˆì •ì„±
    axes[1,0].scatter(human_dyn.transition_complexity, human_stab.convergence_smoothness, 
                     s=100, color='skyblue', label='Human', alpha=0.8)
    axes[1,0].scatter(llm_dyn.transition_complexity, llm_stab.convergence_smoothness,
                     s=100, color='orange', label='LLM', alpha=0.8)
    axes[1,0].set_title('Complexity vs Stability')
    axes[1,0].set_xlabel('Transition Complexity')
    axes[1,0].set_ylabel('Convergence Smoothness')
    axes[1,0].legend()
    axes[1,0].grid(True, alpha=0.3)
    
    # 5. ì˜ˆì¸¡ê°€ëŠ¥ì„± vs ì¼ê´€ì„±
    axes[1,1].scatter(human_dyn.transition_predictability, human_stab.learning_consistency,
                     s=100, color='skyblue', label='Human', alpha=0.8)
    axes[1,1].scatter(llm_dyn.transition_predictability, llm_stab.learning_consistency,
                     s=100, color='orange', label='LLM', alpha=0.8)
    axes[1,1].set_title('Predictability vs Consistency')
    axes[1,1].set_xlabel('Transition Predictability')
    axes[1,1].set_ylabel('Learning Consistency')
    axes[1,1].legend()
    axes[1,1].grid(True, alpha=0.3)
    
    # 6. ì¢…í•© ì ìˆ˜ ë¹„êµ
    human_dynamics_score = np.mean([human_dyn.transition_consistency, human_dyn.transition_predictability, human_dyn.markov_property])
    llm_dynamics_score = np.mean([llm_dyn.transition_consistency, llm_dyn.transition_predictability, llm_dyn.markov_property])
    human_stability_score = np.mean([human_stab.convergence_smoothness, human_stab.learning_consistency, human_stab.gradient_stability])
    llm_stability_score = np.mean([llm_stab.convergence_smoothness, llm_stab.learning_consistency, llm_stab.gradient_stability])
    
    categories = ['Transition\nDynamics', 'Learning\nStability']
    human_scores = [human_dynamics_score, human_stability_score]
    llm_scores = [llm_dynamics_score, llm_stability_score]
    
    x = np.arange(len(categories))
    axes[1,2].bar(x - width/2, human_scores, width, label='Human', color='skyblue', alpha=0.8)
    axes[1,2].bar(x + width/2, llm_scores, width, label='LLM', color='orange', alpha=0.8)
    axes[1,2].set_title('Overall Performance')
    axes[1,2].set_xticks(x)
    axes[1,2].set_xticklabels(categories)
    axes[1,2].set_ylabel('Composite Score')
    axes[1,2].legend()
    axes[1,2].grid(True, alpha=0.3)
    
    plt.tight_layout()
    
    save_path = results_dir / 'comprehensive_dynamics_stability_analysis.png'
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"ì¢…í•© ê²°ê³¼ ì‹œê°í™” ì €ì¥: {save_path}")
    
    plt.show()

def analyze_dynamics_stability_correlation(human_dyn: TransitionDynamicsMetrics,
                                         llm_dyn: TransitionDynamicsMetrics,
                                         human_stab: LearningStabilityMetrics,
                                         llm_stab: LearningStabilityMetrics):
    """ì „ì´ ë™ì—­í•™ê³¼ í•™ìŠµ ì•ˆì •ì„± ê°„ ìƒê´€ê´€ê³„ ë¶„ì„"""
    
    print("ì „ì´ ë™ì—­í•™ - í•™ìŠµ ì•ˆì •ì„± ìƒê´€ê´€ê³„ ë¶„ì„ ì¤‘...")
    
    # ì „ì´ ë™ì—­í•™ ì ìˆ˜ ê³„ì‚°
    def calc_dynamics_score(metrics):
        return (
            metrics.transition_consistency * 0.25 +
            metrics.transition_predictability * 0.25 +
            metrics.transition_stability * 0.20 +
            metrics.markov_property * 0.15 +
            metrics.determinism_score * 0.15
        )
    
    # í•™ìŠµ ì•ˆì •ì„± ì ìˆ˜ ê³„ì‚°
    def calc_stability_score(metrics):
        return (
            metrics.convergence_smoothness * 0.25 +
            metrics.learning_consistency * 0.20 +
            metrics.gradient_stability * 0.20 +
            metrics.initialization_robustness * 0.15 +
            (1.0 / (1.0 + metrics.final_variance)) * 0.20  # ë‚®ì€ ë¶„ì‚°ì´ ì¢‹ìŒ
        )
    
    human_dynamics_score = calc_dynamics_score(human_dyn)
    llm_dynamics_score = calc_dynamics_score(llm_dyn)
    human_stability_score = calc_stability_score(human_stab)
    llm_stability_score = calc_stability_score(llm_stab)
    
    # ìƒê´€ê´€ê³„ ê³„ì‚°
    dynamics_scores = [human_dynamics_score, llm_dynamics_score]
    stability_scores = [human_stability_score, llm_stability_score]
    
    if len(set(dynamics_scores)) > 1 and len(set(stability_scores)) > 1:
        correlation_coeff, p_value = pearsonr(dynamics_scores, stability_scores)
    else:
        correlation_coeff = 0.0
        p_value = 1.0
    
    correlation_results = {
        'human_dynamics_score': human_dynamics_score,
        'llm_dynamics_score': llm_dynamics_score,
        'human_stability_score': human_stability_score,
        'llm_stability_score': llm_stability_score,
        'dynamics_advantage': llm_dynamics_score - human_dynamics_score,
        'stability_advantage': llm_stability_score - human_stability_score,
        'correlation_coefficient': correlation_coeff,
        'p_value': p_value
    }
    
    return correlation_results

def compile_dynamics_stability_results(human_dyn: TransitionDynamicsMetrics,
                                     llm_dyn: TransitionDynamicsMetrics,
                                     human_stab: LearningStabilityMetrics,
                                     llm_stab: LearningStabilityMetrics,
                                     correlation_analysis: Dict):
    """ìµœì¢… ê²°ê³¼ ì»´íŒŒì¼"""
    
    # ê°€ì„¤ ê²€ì¦
    hypothesis_verified = (
        correlation_analysis['dynamics_advantage'] > 0 and
        correlation_analysis['stability_advantage'] > 0
    )
    
    # ì£¼ìš” ê°œì„  ì‚¬í•­
    key_improvements = {
        'transition_dynamics': {
            'consistency': llm_dyn.transition_consistency - human_dyn.transition_consistency,
            'predictability': llm_dyn.transition_predictability - human_dyn.transition_predictability,
            'stability': llm_dyn.transition_stability - human_dyn.transition_stability,
            'markov_property': llm_dyn.markov_property - human_dyn.markov_property
        },
        'learning_stability': {
            'convergence_smoothness': llm_stab.convergence_smoothness - human_stab.convergence_smoothness,
            'learning_consistency': llm_stab.learning_consistency - human_stab.learning_consistency,
            'gradient_stability': llm_stab.gradient_stability - human_stab.gradient_stability,
            'final_variance_reduction': human_stab.final_variance - llm_stab.final_variance
        }
    }
    
    results = {
        'experiment_name': 'LLM annotation â†’ ë” ì¼ê´€ëœ ì „ì´ ë™ì—­í•™ â†’ ë” ì•ˆì •ì  í•™ìŠµ',
        'hypothesis_verified': hypothesis_verified,
        'transition_dynamics_metrics': {
            'human': human_dyn.__dict__,
            'llm': llm_dyn.__dict__
        },
        'learning_stability_metrics': {
            'human': human_stab.__dict__,
            'llm': llm_stab.__dict__
        },
        'correlation_analysis': correlation_analysis,
        'key_improvements': key_improvements,
        'conclusion': (
            "âœ… LLM annotationì´ ë” ì¼ê´€ëœ ì „ì´ ë™ì—­í•™ì„ ì œê³µí•˜ì—¬ ë” ì•ˆì •ì ì¸ í•™ìŠµì„ ë‹¬ì„±í•¨"
            if hypothesis_verified else
            "âŒ ê°€ì„¤ì´ ì¶©ë¶„íˆ ì§€ì§€ë˜ì§€ ì•ŠìŒ"
        )
    }
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"\n{'='*60}")
    print("ì‹¤í—˜ ê²°ê³¼ ìš”ì•½")
    print(f"{'='*60}")
    print(f"ê°€ì„¤ ê²€ì¦: {'âœ… ì„±ê³µ' if hypothesis_verified else 'âŒ ì‹¤íŒ¨'}")
    print(f"ì „ì´ ë™ì—­í•™ ìš°ìœ„: {correlation_analysis['dynamics_advantage']:.3f}")
    print(f"í•™ìŠµ ì•ˆì •ì„± ìš°ìœ„: {correlation_analysis['stability_advantage']:.3f}")
    
    print(f"\nì£¼ìš” ê°œì„  ì‚¬í•­:")
    print(f"  ì „ì´ ì¼ê´€ì„±: +{key_improvements['transition_dynamics']['consistency']:.3f}")
    print(f"  ì „ì´ ì˜ˆì¸¡ê°€ëŠ¥ì„±: +{key_improvements['transition_dynamics']['predictability']:.3f}")
    print(f"  ìˆ˜ë ´ ë¶€ë“œëŸ¬ì›€: +{key_improvements['learning_stability']['convergence_smoothness']:.3f}")
    print(f"  í•™ìŠµ ì¼ê´€ì„±: +{key_improvements['learning_stability']['learning_consistency']:.3f}")
    print(f"  ë¶„ì‚° ê°ì†Œ: +{key_improvements['learning_stability']['final_variance_reduction']:.3f}")
    
    return results

if __name__ == "__main__":
    try:
        results = run_transition_dynamics_pipeline()
        print("\nğŸ‰ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ!")
        print(f"ê²°ê³¼ íŒŒì¼ ìœ„ì¹˜: mdp/rl_tests/transition_dynamics_results/")
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()