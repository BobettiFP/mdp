{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "\n",
    "# 데이터 로딩\n",
    "# with open(\"dialogues_001.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "#     data = json.load(f)\n",
    "\n",
    "# 시퀀스 추출 함수\n",
    "with open(\"/Users/hyegang/Desktop/졸업논문/multiwoz/data/MultiWOZ_2.2/train/dialogues_00.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "def extract_sequence(dialogue):\n",
    "    sequence = []\n",
    "    for turn in dialogue.get(\"turns\", []):\n",
    "        if turn[\"speaker\"] == \"USER\":\n",
    "            actions = []\n",
    "            for frame in turn.get(\"frames\", []):\n",
    "                intent = frame.get(\"state\", {}).get(\"active_intent\", \"\")\n",
    "                slots = frame.get(\"state\", {}).get(\"slot_values\", {})\n",
    "                for slot, values in slots.items():\n",
    "                    for value in values:\n",
    "                        actions.append(f\"{intent.upper()}({slot}={value})\")\n",
    "            if actions:\n",
    "                sequence.append(\" + \".join(actions))\n",
    "    return \" → \".join(sequence)\n",
    "\n",
    "dialogue_ids, sequences = [], []\n",
    "for d in data:\n",
    "    seq = extract_sequence(d)\n",
    "    if seq:\n",
    "        dialogue_ids.append(d[\"dialogue_id\"])\n",
    "        sequences.append(seq)\n",
    "\n",
    "df = pd.DataFrame({\"dialogue_id\": dialogue_ids, \"sequence\": sequences})\n",
    "\n",
    "# 임베딩\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "embeddings = model.encode(df[\"sequence\"].tolist(), convert_to_tensor=True)\n",
    "\n",
    "# 유사도 기반 거리행렬 → 클러스터링\n",
    "cos_sim_matrix = (embeddings @ embeddings.T).cpu().numpy()\n",
    "distance_matrix = 1 - cos_sim_matrix\n",
    "clustering = AgglomerativeClustering(n_clusters=None, distance_threshold=0.3, linkage='average', metric='precomputed')\n",
    "labels = clustering.fit_predict(distance_matrix)\n",
    "df[\"cluster\"] = labels\n",
    "\n",
    "# t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=5)\n",
    "reduced = tsne.fit_transform(embeddings.cpu().numpy())\n",
    "\n",
    "# plotly 시각화\n",
    "plot_df = pd.DataFrame({\n",
    "    \"x\": reduced[:, 0],\n",
    "    \"y\": reduced[:, 1],\n",
    "    \"dialogue_id\": df[\"dialogue_id\"],\n",
    "    \"cluster\": df[\"cluster\"].astype(str),\n",
    "    \"sequence\": df[\"sequence\"]\n",
    "})\n",
    "\n",
    "fig = px.scatter(\n",
    "    plot_df,\n",
    "    x=\"x\",\n",
    "    y=\"y\",\n",
    "    color=\"cluster\",\n",
    "    hover_data=[\"dialogue_id\", \"sequence\"],\n",
    "    title=\"t-SNE of MDP Dialogue Sequences (Interactive)\"\n",
    ")\n",
    "\n",
    "# HTML 저장\n",
    "fig.write_html(\"tsne_mdp_sequences.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 시퀀스 줄바꿈 함수\n",
    "def shorten_sequence(seq, max_len=60):\n",
    "    return \"<br>\".join([seq[i:i+max_len] for i in range(0, len(seq), max_len)])\n",
    "\n",
    "# 1. 시퀀스 임베딩\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "embeddings = model.encode(df[\"sequence\"].tolist(), convert_to_tensor=True)\n",
    "\n",
    "# 2. t-SNE 차원 축소\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=5)\n",
    "reduced = tsne.fit_transform(embeddings.cpu().numpy())\n",
    "\n",
    "# 3. plot용 데이터프레임 구성\n",
    "plot_df = pd.DataFrame({\n",
    "    \"x\": reduced[:, 0],\n",
    "    \"y\": reduced[:, 1],\n",
    "    \"dialogue_id\": df[\"dialogue_id\"],\n",
    "    \"cluster\": df[\"cluster\"].astype(str),\n",
    "    \"sequence\": df[\"sequence\"]\n",
    "})\n",
    "plot_df[\"short_sequence\"] = plot_df[\"sequence\"].apply(lambda x: shorten_sequence(x))\n",
    "\n",
    "# 4. Plotly 시각화\n",
    "fig = px.scatter(\n",
    "    plot_df,\n",
    "    x=\"x\",\n",
    "    y=\"y\",\n",
    "    color=\"cluster\",\n",
    "    hover_data={\"dialogue_id\": True, \"short_sequence\": True, \"sequence\": False},\n",
    "    title=\"t-SNE of MDP Dialogue Sequences (Interactive)\"\n",
    ")\n",
    "\n",
    "# 5. HTML로 저장\n",
    "fig.write_html(\"tsne_mdp_sequences_wrapped.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 17/17 [00:01<00:00, 14.11it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import plotly.express as px\n",
    "\n",
    "# 1. JSON 파일 불러오기\n",
    "data_dir = \"/Users/hyegang/Desktop/졸업논문/multiwoz/data/MultiWOZ_2.2/train\"\n",
    "dialogues = []\n",
    "\n",
    "for file_name in tqdm(os.listdir(data_dir)):\n",
    "    if file_name.endswith(\".json\"):\n",
    "        with open(os.path.join(data_dir, file_name), \"r\", encoding=\"utf-8\") as f:\n",
    "            dialogues.extend(json.load(f))\n",
    "\n",
    "# 2. MDP 시퀀스 추출 함수\n",
    "def extract_sequence(dialogue):\n",
    "    sequence = []\n",
    "    for turn in dialogue.get(\"turns\", []):\n",
    "        if turn[\"speaker\"] == \"USER\":\n",
    "            actions = []\n",
    "            for frame in turn.get(\"frames\", []):\n",
    "                intent = frame.get(\"state\", {}).get(\"active_intent\", \"\")\n",
    "                slots = frame.get(\"state\", {}).get(\"slot_values\", {})\n",
    "                for slot, values in slots.items():\n",
    "                    for value in values:\n",
    "                        actions.append(f\"{intent.upper()}({slot}={value})\")\n",
    "            if actions:\n",
    "                sequence.append(\" + \".join(actions))\n",
    "    return \" → \".join(sequence)\n",
    "\n",
    "# 3. 시퀀스 리스트 생성\n",
    "dialogue_ids, sequences = [], []\n",
    "for d in dialogues:\n",
    "    seq = extract_sequence(d)\n",
    "    if seq:\n",
    "        dialogue_ids.append(d[\"dialogue_id\"])\n",
    "        sequences.append(seq)\n",
    "\n",
    "df = pd.DataFrame({\"dialogue_id\": dialogue_ids, \"sequence\": sequences})\n",
    "\n",
    "# 4. Sentence-BERT 임베딩\n",
    "model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "embeddings = model.encode(df[\"sequence\"].tolist(), convert_to_tensor=True)\n",
    "\n",
    "# 5. 거리 행렬 + 클러스터링\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "cos_sim_matrix = cosine_similarity(embeddings.cpu())\n",
    "distance_matrix = 1 - cos_sim_matrix\n",
    "\n",
    "clustering = AgglomerativeClustering(n_clusters=20, linkage='average', metric='precomputed')\n",
    "labels = clustering.fit_predict(distance_matrix)\n",
    "df[\"cluster\"] = labels\n",
    "\n",
    "# 6. t-SNE\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "reduced = tsne.fit_transform(embeddings.cpu().numpy())\n",
    "\n",
    "# 7. Plotly 시각화\n",
    "def shorten_sequence(seq, max_len=80):\n",
    "    return \"<br>\".join([seq[i:i+max_len] for i in range(0, len(seq), max_len)])\n",
    "\n",
    "plot_df = pd.DataFrame({\n",
    "    \"x\": reduced[:, 0],\n",
    "    \"y\": reduced[:, 1],\n",
    "    \"dialogue_id\": df[\"dialogue_id\"],\n",
    "    \"cluster\": df[\"cluster\"].astype(str),\n",
    "    \"sequence\": df[\"sequence\"]\n",
    "})\n",
    "plot_df[\"short_sequence\"] = plot_df[\"sequence\"].apply(shorten_sequence)\n",
    "\n",
    "fig = px.scatter(\n",
    "    plot_df,\n",
    "    x=\"x\",\n",
    "    y=\"y\",\n",
    "    color=\"cluster\",\n",
    "    hover_data={\"dialogue_id\": True, \"short_sequence\": True, \"sequence\": False},\n",
    "    title=\"t-SNE of MDP Dialogue Sequences from MultiWOZ (Interactive)\"\n",
    ")\n",
    "\n",
    "# 8. 저장\n",
    "fig.write_html(\"tsne_mdp_sequences_multiwoz.html\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster</th>\n",
       "      <th>sequence</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>FIND_ATTRACTION(attraction-name=people's portr...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>FIND_ATTRACTION(attraction-type=museum) → FIND...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>FIND_ATTRACTION(attraction-area=centre) → FIND...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>FIND_ATTRACTION(attraction-type=museum) → FIND...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>FIND_ATTRACTION(attraction-area=centre) → FIND...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>FIND_ATTRACTION(attraction-type=museum) → FIND...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>FIND_TAXI(taxi-destination=golden house) + FIN...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>FIND_TAXI(taxi-departure=ali baba) + FIND_TAXI...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>10</td>\n",
       "      <td>FIND_TAXI(taxi-destination=cambridge lodge res...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>11</td>\n",
       "      <td>FIND_HOTEL(hotel-name=kirkwood house) → FIND_H...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>FIND_HOTEL(hotel-name=lovell lodge) → FIND_HOT...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>FIND_HOTEL(hotel-name=worth house) → FIND_HOTE...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>FIND_RESTAURANT(restaurant-name=city stop rest...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12</td>\n",
       "      <td>FIND_RESTAURANT(restaurant-food=vegetarian) → ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>12</td>\n",
       "      <td>FIND_RESTAURANT(restaurant-food=polish) → FIND...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>13</td>\n",
       "      <td>FIND_RESTAURANT(restaurant-name=the missing so...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>13</td>\n",
       "      <td>FIND_RESTAURANT(restaurant-name=the missing sock)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>14</td>\n",
       "      <td>FIND_HOSPITAL(hospital-department=john farman ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>14</td>\n",
       "      <td>FIND_HOSPITAL(hospital-department=neurology) →...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>14</td>\n",
       "      <td>FIND_TAXI(taxi-destination=the hospital)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>15</td>\n",
       "      <td>FIND_TAXI(taxi-departure=sheep's green and lam...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>16</td>\n",
       "      <td>FIND_RESTAURANT(restaurant-name=la margherita)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>17</td>\n",
       "      <td>FIND_ATTRACTION(attraction-name=the man on the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>18</td>\n",
       "      <td>FIND_ATTRACTION(attraction-name=cambridge univ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>19</td>\n",
       "      <td>FIND_TRAIN(train-departure=cambridge)</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>FIND_RESTAURANT(restaurant-area=north) + FIND_...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>FIND_RESTAURANT(restaurant-area=east) + FIND_R...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2</td>\n",
       "      <td>FIND_RESTAURANT(restaurant-area=north) + FIND_...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3</td>\n",
       "      <td>FIND_HOTEL(hotel-name=leverton house) → BOOK_H...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3</td>\n",
       "      <td>FIND_TRAIN(train-destination=cambridge) + FIND...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3</td>\n",
       "      <td>FIND_HOTEL(hotel-pricerange=cheap) + FIND_HOTE...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4</td>\n",
       "      <td>FIND_HOTEL(hotel-pricerange=moderate) + FIND_H...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>4</td>\n",
       "      <td>FIND_HOTEL(hotel-internet=yes) + FIND_HOTEL(ho...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>4</td>\n",
       "      <td>FIND_HOTEL(hotel-area=north) → FIND_HOTEL(hote...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>5</td>\n",
       "      <td>FIND_HOSPITAL(hospital-department=paediatric d...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>5</td>\n",
       "      <td>FIND_HOSPITAL(hospital-department=emergency de...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>6</td>\n",
       "      <td>FIND_HOSPITAL(hospital-department=transplant h...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>6</td>\n",
       "      <td>FIND_HOSPITAL(hospital-department=emergency de...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>6</td>\n",
       "      <td>FIND_HOSPITAL(hospital-department=antenatal) →...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>7</td>\n",
       "      <td>FIND_TRAIN(train-destination=broxbourne) → FIN...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>7</td>\n",
       "      <td>FIND_TRAIN(train-destination=cambridge) → FIND...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>7</td>\n",
       "      <td>FIND_TRAIN(train-destination=london liverpool ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>8</td>\n",
       "      <td>FIND_HOSPITAL(hospital-department=hepatobillar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>8</td>\n",
       "      <td>FIND_HOSPITAL(hospital-department=hepatobillar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>9</td>\n",
       "      <td>BOOK_TRAIN(train-arriveby=10:30) + BOOK_TRAIN(...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cluster                                           sequence  count\n",
       "0        0  FIND_ATTRACTION(attraction-name=people's portr...      2\n",
       "1        0  FIND_ATTRACTION(attraction-type=museum) → FIND...      2\n",
       "2        0  FIND_ATTRACTION(attraction-area=centre) → FIND...      2\n",
       "3        1  FIND_ATTRACTION(attraction-type=museum) → FIND...      2\n",
       "4        1  FIND_ATTRACTION(attraction-area=centre) → FIND...      2\n",
       "5        1  FIND_ATTRACTION(attraction-type=museum) → FIND...      2\n",
       "6       10  FIND_TAXI(taxi-destination=golden house) + FIN...      1\n",
       "7       10  FIND_TAXI(taxi-departure=ali baba) + FIND_TAXI...      1\n",
       "8       10  FIND_TAXI(taxi-destination=cambridge lodge res...      1\n",
       "9       11  FIND_HOTEL(hotel-name=kirkwood house) → FIND_H...      2\n",
       "10      11  FIND_HOTEL(hotel-name=lovell lodge) → FIND_HOT...      2\n",
       "11      11  FIND_HOTEL(hotel-name=worth house) → FIND_HOTE...      1\n",
       "12      12  FIND_RESTAURANT(restaurant-name=city stop rest...      1\n",
       "13      12  FIND_RESTAURANT(restaurant-food=vegetarian) → ...      1\n",
       "14      12  FIND_RESTAURANT(restaurant-food=polish) → FIND...      1\n",
       "15      13  FIND_RESTAURANT(restaurant-name=the missing so...      1\n",
       "16      13  FIND_RESTAURANT(restaurant-name=the missing sock)      1\n",
       "17      14  FIND_HOSPITAL(hospital-department=john farman ...      1\n",
       "18      14  FIND_HOSPITAL(hospital-department=neurology) →...      1\n",
       "19      14           FIND_TAXI(taxi-destination=the hospital)      1\n",
       "20      15  FIND_TAXI(taxi-departure=sheep's green and lam...      1\n",
       "21      16     FIND_RESTAURANT(restaurant-name=la margherita)      1\n",
       "22      17  FIND_ATTRACTION(attraction-name=the man on the...      1\n",
       "23      18  FIND_ATTRACTION(attraction-name=cambridge univ...      1\n",
       "24      19              FIND_TRAIN(train-departure=cambridge)      1\n",
       "25       2  FIND_RESTAURANT(restaurant-area=north) + FIND_...      6\n",
       "26       2  FIND_RESTAURANT(restaurant-area=east) + FIND_R...      5\n",
       "27       2  FIND_RESTAURANT(restaurant-area=north) + FIND_...      5\n",
       "28       3  FIND_HOTEL(hotel-name=leverton house) → BOOK_H...      2\n",
       "29       3  FIND_TRAIN(train-destination=cambridge) + FIND...      2\n",
       "30       3  FIND_HOTEL(hotel-pricerange=cheap) + FIND_HOTE...      2\n",
       "31       4  FIND_HOTEL(hotel-pricerange=moderate) + FIND_H...      2\n",
       "32       4  FIND_HOTEL(hotel-internet=yes) + FIND_HOTEL(ho...      2\n",
       "33       4  FIND_HOTEL(hotel-area=north) → FIND_HOTEL(hote...      1\n",
       "34       5  FIND_HOSPITAL(hospital-department=paediatric d...      1\n",
       "35       5  FIND_HOSPITAL(hospital-department=emergency de...      1\n",
       "36       6  FIND_HOSPITAL(hospital-department=transplant h...      4\n",
       "37       6  FIND_HOSPITAL(hospital-department=emergency de...      3\n",
       "38       6  FIND_HOSPITAL(hospital-department=antenatal) →...      2\n",
       "39       7  FIND_TRAIN(train-destination=broxbourne) → FIN...      1\n",
       "40       7  FIND_TRAIN(train-destination=cambridge) → FIND...      1\n",
       "41       7  FIND_TRAIN(train-destination=london liverpool ...      1\n",
       "42       8  FIND_HOSPITAL(hospital-department=hepatobillar...      1\n",
       "43       8  FIND_HOSPITAL(hospital-department=hepatobillar...      1\n",
       "44       9  BOOK_TRAIN(train-arriveby=10:30) + BOOK_TRAIN(...      1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 클러스터별 대표 시퀀스 보기\n",
    "top_sequences_per_cluster = (\n",
    "    plot_df.groupby(\"cluster\")[\"sequence\"]\n",
    "    .apply(lambda x: x.value_counts().head(3))\n",
    "    .reset_index()\n",
    "    .rename(columns={\"level_1\": \"sequence\", \"sequence\": \"count\"})\n",
    ")\n",
    "\n",
    "# 결과 출력\n",
    "top_sequences_per_cluster\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_sequences_per_cluster.to_excel('top_sequences2.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'intents'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/졸업논문/.venv/lib/python3.13/site-packages/pandas/core/indexes/base.py:3805\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3804\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3805\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3806\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mindex.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'intents'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     24\u001b[39m example_path.mkdir(parents=\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# intent 구조 예시 저장\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33mintent_sequence\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mintents\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m.apply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[33m\"\u001b[39m\u001b[33m → \u001b[39m\u001b[33m\"\u001b[39m.join(x))\n\u001b[32m     28\u001b[39m save_examples_by_group(df, \u001b[33m\"\u001b[39m\u001b[33mintent_sequence\u001b[39m\u001b[33m\"\u001b[39m, example_path / \u001b[33m\"\u001b[39m\u001b[33mintent_examples.xlsx\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# 행동 흐름 예시 저장 (sequence 자체 기준)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/졸업논문/.venv/lib/python3.13/site-packages/pandas/core/frame.py:4102\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4101\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4102\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4103\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4104\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/졸업논문/.venv/lib/python3.13/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3807\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3808\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3809\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3810\u001b[39m     ):\n\u001b[32m   3811\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3814\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3815\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3816\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3817\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'intents'"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "save_path = Path(\"/Users/hyegang/Desktop/mdp_classification_results\")\n",
    "def save_examples_by_group(df, group_col, filename, max_groups=10, samples_per_group=5):\n",
    "    examples = []\n",
    "    group_counts = df[group_col].value_counts().head(max_groups)\n",
    "\n",
    "    for group in group_counts.index:\n",
    "        group_rows = df[df[group_col] == group]\n",
    "        sample_rows = group_rows.sample(min(samples_per_group, len(group_rows)), random_state=42)\n",
    "        for _, row in sample_rows.iterrows():\n",
    "            examples.append({\n",
    "                group_col: group,\n",
    "                \"dialogue_id\": row[\"dialogue_id\"],\n",
    "                \"sequence\": row[\"sequence\"]\n",
    "            })\n",
    "\n",
    "    example_df = pd.DataFrame(examples)\n",
    "    example_df.to_excel(filename, index=False)\n",
    "\n",
    "# 예시 저장 경로\n",
    "example_path = save_path / \"examples\"\n",
    "\n",
    "example_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# intent 구조 예시 저장\n",
    "df[\"intent_sequence\"] = df[\"intents\"].apply(lambda x: \" → \".join(x))\n",
    "save_examples_by_group(df, \"intent_sequence\", example_path / \"intent_examples.xlsx\")\n",
    "\n",
    "# 행동 흐름 예시 저장 (sequence 자체 기준)\n",
    "save_examples_by_group(df, \"sequence\", example_path / \"action_flow_examples.xlsx\")\n",
    "\n",
    "# 슬롯 구조 예시 저장\n",
    "df[\"slot_signature\"] = df[\"slot_types\"].apply(lambda x: \", \".join(sorted(x)))\n",
    "save_examples_by_group(df, \"slot_signature\", example_path / \"slot_structure_examples.xlsx\")\n",
    "\n",
    "# 도메인 구조 예시 저장\n",
    "df[\"domain_signature\"] = df[\"domains\"].apply(lambda x: \", \".join(sorted(x)))\n",
    "save_examples_by_group(df, \"domain_signature\", example_path / \"domain_examples.xlsx\")\n",
    "\n",
    "# 대화 길이 그룹 예시 저장\n",
    "df[\"turn_length_bucket\"] = pd.cut(df[\"turn_length\"], bins=[0, 4, 8, 15, 100], labels=[\"short\", \"medium\", \"long\", \"very long\"])\n",
    "save_examples_by_group(df, \"turn_length_bucket\", example_path / \"turn_length_examples.xlsx\")\n",
    "\n",
    "# 성공 여부 예시 저장\n",
    "df[\"success_label\"] = df[\"task_success\"].apply(lambda x: \"success\" if x else \"failure\")\n",
    "save_examples_by_group(df, \"success_label\", example_path / \"task_success_examples.xlsx\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pydotplus'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolors\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmcolors\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StringIO\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpydotplus\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# 예제 데이터셋 생성\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pydotplus'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "top_sequences_per_cluster.to_excel(\"top_sequences.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openpyxl\n",
      "  Downloading openpyxl-3.1.5-py2.py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting et-xmlfile (from openpyxl)\n",
      "  Downloading et_xmlfile-2.0.0-py3-none-any.whl.metadata (2.7 kB)\n",
      "Downloading openpyxl-3.1.5-py2.py3-none-any.whl (250 kB)\n",
      "Downloading et_xmlfile-2.0.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: et-xmlfile, openpyxl\n",
      "Successfully installed et-xmlfile-2.0.0 openpyxl-3.1.5\n"
     ]
    }
   ],
   "source": [
    "!pip3 install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting plotly\n",
      "  Downloading plotly-6.0.1-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting narwhals>=1.15.1 (from plotly)\n",
      "  Downloading narwhals-1.32.0-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: packaging in /Users/hyegang/Desktop/졸업논문/.venv/lib/python3.13/site-packages (from plotly) (24.2)\n",
      "Downloading plotly-6.0.1-py3-none-any.whl (14.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading narwhals-1.32.0-py3-none-any.whl (320 kB)\n",
      "Installing collected packages: narwhals, plotly\n",
      "Successfully installed narwhals-1.32.0 plotly-6.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip3 install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentence-transformers\n",
      "  Using cached sentence_transformers-3.4.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.6.1-cp313-cp313-macosx_12_0_arm64.whl.metadata (31 kB)\n",
      "Requirement already satisfied: matplotlib in /Users/hyegang/Desktop/졸업논문/.venv/lib/python3.13/site-packages (3.10.1)\n",
      "Collecting seaborn\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting transformers<5.0.0,>=4.41.0 (from sentence-transformers)\n",
      "  Using cached transformers-4.50.1-py3-none-any.whl.metadata (39 kB)\n",
      "Requirement already satisfied: tqdm in /Users/hyegang/Desktop/졸업논문/.venv/lib/python3.13/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /Users/hyegang/Desktop/졸업논문/.venv/lib/python3.13/site-packages (from sentence-transformers) (2.6.0)\n",
      "Requirement already satisfied: scipy in /Users/hyegang/Desktop/졸업논문/.venv/lib/python3.13/site-packages (from sentence-transformers) (1.15.2)\n",
      "Collecting huggingface-hub>=0.20.0 (from sentence-transformers)\n",
      "  Using cached huggingface_hub-0.29.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: Pillow in /Users/hyegang/Desktop/졸업논문/.venv/lib/python3.13/site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Users/hyegang/Desktop/졸업논문/.venv/lib/python3.13/site-packages (from scikit-learn) (2.2.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/hyegang/Desktop/졸업논문/.venv/lib/python3.13/site-packages (from scikit-learn) (1.4.2)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/hyegang/Desktop/졸업논문/.venv/lib/python3.13/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/hyegang/Desktop/졸업논문/.venv/lib/python3.13/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/hyegang/Desktop/졸업논문/.venv/lib/python3.13/site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/hyegang/Desktop/졸업논문/.venv/lib/python3.13/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/hyegang/Desktop/졸업논문/.venv/lib/python3.13/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/hyegang/Desktop/졸업논문/.venv/lib/python3.13/site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/hyegang/Desktop/졸업논문/.venv/lib/python3.13/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pandas>=1.2 in /Users/hyegang/Desktop/졸업논문/.venv/lib/python3.13/site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: filelock in /Users/hyegang/Desktop/졸업논문/.venv/lib/python3.13/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/hyegang/Desktop/졸업논문/.venv/lib/python3.13/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
      "Collecting pyyaml>=5.1 (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached PyYAML-6.0.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
      "Collecting requests (from huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/hyegang/Desktop/졸업논문/.venv/lib/python3.13/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/hyegang/Desktop/졸업논문/.venv/lib/python3.13/site-packages (from pandas>=1.2->seaborn) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/hyegang/Desktop/졸업논문/.venv/lib/python3.13/site-packages (from pandas>=1.2->seaborn) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/hyegang/Desktop/졸업논문/.venv/lib/python3.13/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: networkx in /Users/hyegang/Desktop/졸업논문/.venv/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/hyegang/Desktop/졸업논문/.venv/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in /Users/hyegang/Desktop/졸업논문/.venv/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (77.0.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/hyegang/Desktop/졸업논문/.venv/lib/python3.13/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/hyegang/Desktop/졸업논문/.venv/lib/python3.13/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/hyegang/Desktop/졸업논문/.venv/lib/python3.13/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers<5.0.0,>=4.41.0->sentence-transformers)\n",
      "  Downloading safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/hyegang/Desktop/졸업논문/.venv/lib/python3.13/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Collecting charset-normalizer<4,>=2 (from requests->huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached charset_normalizer-3.4.1-cp313-cp313-macosx_10_13_universal2.whl.metadata (35 kB)\n",
      "Collecting idna<4,>=2.5 (from requests->huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests->huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests->huggingface-hub>=0.20.0->sentence-transformers)\n",
      "  Using cached certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Using cached sentence_transformers-3.4.1-py3-none-any.whl (275 kB)\n",
      "Downloading scikit_learn-1.6.1-cp313-cp313-macosx_12_0_arm64.whl (11.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.1/11.1 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Downloading huggingface_hub-0.29.3-py3-none-any.whl (468 kB)\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Downloading transformers-4.50.1-py3-none-any.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading PyYAML-6.0.2-cp313-cp313-macosx_11_0_arm64.whl (171 kB)\n",
      "Downloading safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl (418 kB)\n",
      "Downloading tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Using cached charset_normalizer-3.4.1-cp313-cp313-macosx_10_13_universal2.whl (195 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Installing collected packages: urllib3, threadpoolctl, safetensors, pyyaml, idna, charset-normalizer, certifi, scikit-learn, requests, seaborn, huggingface-hub, tokenizers, transformers, sentence-transformers\n",
      "Successfully installed certifi-2025.1.31 charset-normalizer-3.4.1 huggingface-hub-0.29.3 idna-3.10 pyyaml-6.0.2 requests-2.32.3 safetensors-0.5.3 scikit-learn-1.6.1 seaborn-0.13.2 sentence-transformers-3.4.1 threadpoolctl-3.6.0 tokenizers-0.21.1 transformers-4.50.1 urllib3-2.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install sentence-transformers scikit-learn matplotlib seaborn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
